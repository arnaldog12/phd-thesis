\section{Materials and Methods} \label{sec:method}
 
This chapter details the materials used in this work and the method proposed in this thesis, called \methodname. We begin by describing how the dataset have been gathered to train the neural network model and present an analysis of the final database. Later, the proposed method is thoroughly explained, including the main idea behind the architectural design, its components, and the implementation details. The results and other analyses of the proposed work are reserved for Chapter \ref{sec:results}.
 
\subsection{Datasets}
 
In this section, the databases used in this study are described. First, the official databases used by \acs{ficv} to benchmark algorithms that assess the \icao standard is presented. Subsequently, an expansion of this database used to train the proposed method. Finally, statistics about the database are detailed and discussed.
 
\subsubsection{\acs{ficv} Dataset} \label{sec:database}
 
As mentioned in the literature review (see Chapter \ref{sec:literature}), one of the challenges behind the \icao standard is the lack of fully labeled datasets for all 23 requirements. According to our previous investigation, the only dataset publicly available for research is the one provided by the \acs{ficv} competition for its participants. The full dataset comprises 5588 images from 601 subjects gathered from different sources. It was built \adhoc using images from public databases, and additional images were manually acquired to cover some missing requirements. The image distribution of the \acs{ficv} dataset is as follows:
 
\begin{itemize}
\item 1741 images from the AR database \citep{martinez1998ar} of size 768$\times$576 pixels;
\item 1935 images from the FRGC database \citep{databaseFRGC} of sizes 1704$\times$2272 or 1200$\times$1600 pixels;
\item 291 images from the PUT database \citep{kasinski2008put} of size 2048$\times$1536 pixels;
\item 804 images artificially generated by applying ink-marked/creased, pixelation, and washed out effects to compliant images from the AR database; and
\item 817 newly acquired images of size 1600$\times$1200 pixels.
\end{itemize}
 
Moreover, the following information is given for each image:
 
\begin{itemize}
\item the \textbf{coordinates of the eye corners} expressed by four pairs of $(x, y)$ coordinates (two for each eye);
\item the \textbf{compliance to the photographic and pose requirements} expressed as one of three possible values: 
 \begin{enumerate}[i]
 \item \textbf{compliant}: when a specific requirement is declared compliant for an image, it means that such image is acceptable for that characteristic. In theory, only fully compliant images should be considered for a later face-recognition process. In the ground truth, compliance is represented by integer 1.
 \item \textbf{non-compliant}: represented by the label 0 in the ground-truth, means the opposite of compliant, i.e., for a given image, the specific requirement is not-acceptable; and
 \item \textbf{dummy}: used for uncertainty cases. For example, when a person wears glasses with dark-tinted lenses, it is difficult to evaluate whether the eyes are open, even for human experts. It is represented by an integer $-1$ in the ground truth.
 \end{enumerate}
\end{itemize}
 
In total, 310 images are fully compliant (i.e., compliant with all the requirements), and 5278 images have one or more requirements that are non-compliant. A representative subset of 720 images, called \ficvtest, is publicly available to the participants of the \acs{ficv} competition, and can be used for parameter setup and training. It contains 50 fully compliant images and 670 images not compliant with one or more characteristics. The remaining images are used as the private image set reserved for the benchmark of the submitted algorithms. The private database is referred to as \ficvofficial by the \acs{ficv}. In this thesis, we refer to this as the official dataset of the \acs{ficv} or \fvcongoing. 
 
Since some of the images in \ficvtest belong to public datasets and cannot be directly distributed to third parties, only the ground-truth data of each image is provided for the participants. Nevertheless, \fvcongoing provides a utility tool with instructions to generate the training set. Therefore, participants must first download the images of the public datasets (i.e., AR, FRGC, and PUT) from their respective websites and then use the utility tool to produce the training set. However, using this tool, we obtained only 571 annotated images out of the 720 images. We contacted the person responsible for the \acs{ficv} competition, but it was informed that the remaining images belong to the set of images acquired internally, and these could not be shared.
 
\autoref{tab:req-dist-ficv-test} shows the distribution of images for each requirement in the \ficvtest database. It is possible to see that some requirements do not have non-compliant or dummy images (e.g., \inkmarked or \framestooheavy). Therefore, we increased the \ficvtest dataset using an \adhoc approach. Further details are provided in the following subsection.
 
\input{tables/table_req_dist_ficvtest}
 
As indicated on the \acs{ficv} competition webpage, there are dependencies between specific requirements. For instance, when a person is wearing a veil (i.e., \veiloverface is non-compliant), evaluating the requirement \mouthopen may be impossible. In this case, this requirement would be considered as a dummy. Therefore, we analyzed the \ficvtest dataset to determine the dependencies between non-compliant and dummy requirements. A diagram of these dependencies is shown in \autoref{fig:icao_dependencies}.
 
\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{images/dataset/icao_dependencies.pdf}
\caption{Diagram of dependencies between non-compliant and dummy requirements. An arrow indicates that when the parent requirement is labeled non-compliant, the childrenâ€™s requirements are considered dummy. A light gray arrow indicates that such a relationship is not always true. Source: own elaboration.}
\label{fig:icao_dependencies}
\end{figure}
 
\subsubsection{\textit{Ad hoc} Dataset} \label{sec:databaseadhoc}
 
Usually, \acl{dl}-based methods require a large dataset for learning, primarily if training is performed from scratch. Techniques like Transfer Learning or Data Augmentation can be employed for low-size datasets. However, they presume some assumptions. For example, Transfer Learning works well when the original network was trained in a similar domain to the new problem. Similarly, we need to have significant intraclass variance in the samples to improve the results with data augmentation. Moreover, traditional Data Augmentation does not change the distribution of labels in a dataset because random transformations are uniformly applied. 
 
These problems can be found in the \ficvtest dataset. First, the \icao standard represents a unique type of problem, and thus, Transfer Learning may not be a valid option. Although some face datasets are available for research, Neural Networks are typically trained for face recognition in these datasets. Hence, although some features learned by these networks might be helpful for ICAO, some requirements are not present in these databases (e.g., \inkmarked) or may be ignored by the network (e.g., \variedbackground). Finally, Data Augmentation might not help with the \ficvtest database due to the high imbalance in specific requirements. Also, some transformations applied by Data Augmentation procedures can affect some requirements. For example, random brightness may affect the \toodarklight requirement, and random rotations can change the angles of the face and disturb \rollpitchyaw.
 
With only 571 images available in the \ficvtest database for all 23 requirements, we decided to increase the dataset by following a procedure similar to that adopted by \acs{ficv}. Therefore, we manually gathered additional images from the AR, FRGC, and PUT databases. We also included images from the AFW database \citep{databaseAFW} and acquired new images according to the \icao requirements. In this case, we created a dataset of pictures by asking hundreds of volunteers to reproduce the characteristics of non-compliant photographic requirements (\#8--\#30 of Table \ref{tab:icao}). The images were later annotated by three people from our research group, specially trained for this task. As in \citep{Nowak2010}, we provided an annotation tool in which the task of annotators was to select the compliance level of each requirement: compliant (C), non-compliant (NC) or Dummy (D). It was developed with Pybossa\footnote{https://pybossa.com} and provided as a web application tool. To produce the final labels, we followed an approach similar to that used by \citep{Chang2017}. Thus, expert annotators compared and discussed conflicting labels in an iterative and collaborative workflow. In total, we ended up with a training set of 5763 images. The image distribution per dataset is defined as follows:
 
\begin{itemize}
\item 22 images from AFW database of sizes from 362$\times$362 to 1984$\times$1984 pixels;
\item 1368 images from AR database;
\item 50 images from PUT database;
\item 1772 images from FRGC database; and
\item 2551 newly acquired images of sizes from 976$\times$1301 to 4608$\times$3456 pixels
\end{itemize}
 
Samples from each database can be seen in \autoref{fig:dataset_samples}. Our dataset has 177 fully compliant images and 5586 images with one or more non-compliant requirements. One crucial detail to notice is that, although the \adhoc dataset has the label dummy in the ground truth, we decided to merge dummy and non-compliant labels. This was done for two reasons. First, according to the \acs{ficv} protocol, only compliant and non-compliant images are considered for the benchmark of each requirement (see Section \ref{sec:fvcongoing} of Chapter \ref{sec:background}). Furthermore, the dataset imbalance is diminished by combining these two labels, and the intra-class variance of non-compliant images is increased. Therefore, this can help the network define a better decision boundary between compliant and non-compliant requirements and improve performance. Our results corroborated this hypothesis. However, this decision aims at the competition of the \acs{ficv}. In other contexts, the dummy information may be relevant (e.g., eye detection in case of dark glasses may be avoidable and lead to wrong analysis). Finally, the distribution of labels per requirement can be seen in \autoref{tab:req-dist}. 
 
\begin{figure}[tb]
\centering
\includegraphics[width=0.5\linewidth]{images/dataset/dataset_samples.pdf}
\caption{Samples of images in our dataset. The first four rows contain samples of the AFW \citep{databaseAFW}, AR \citep{martinez1998ar}, FRGC \citep{databaseFRGC}, and PUT \citep{kasinski2008put} databases, respectively. The last two rows are samples of images we acquired for this work.}
\label{fig:dataset_samples}
\end{figure}
 
\input{tables/table_req_dist}
 
Since the \acs{ficv} competition also evaluates the capacity of the algorithm to locate the eye's centers, we also had to label them in our \textit{Ad hoc} dataset. It was accomplished using the Computer Vision library called Dlib \citep{dlib}, which contains a built-in shape predictor for 68 face landmarks. When submitted to \acs{ficv} competition, the detection of eye landmarks by Dlib achieved $d_{eye} \in [0;0.1[\ = 94.13\%$ in the \ficvtest dataset (see \autoref{eq:icao-eyes}). Thus, it has a high level of effectiveness and defines an upper bound for our landmark localization predictor.
 
To better understand the \adhoc dataset, we analyzed the images and labels. Such analyses are better described in the following subsection.
 
\subsubsection{\textit{Ad hoc} Dataset Analysis}
 
\autoref{fig:labels_by_sample} shows the number of images with a certain number of compliant/non-compliant labels in the \adhoc dataset. As mentioned, there are 177 fully compliant images (i.e., the number of non-compliant labels equals zero). In addition, most images have 2 to 6 non-compliant requirements, but there are images with up to 14 non-compliant requirements.
 
\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{images/dataset/labels_by_sample.pdf}
\caption{Number of images according to the count of compliant/non-compliant labels. Source: own elaboration.}
\label{fig:labels_by_sample}
\end{figure*}
 
\autoref{fig:reqs_distribution} contains the label distribution by requirement presented earlier in \autoref{tab:req-dist}. As can be seen, the requirements \inkmarked, \washedout, \framestooheavy, and \otherfacesortoys are the most unbalanced. Also, the \unnaturalskintone is the only requirement in which the number of non-compliant samples exceeds the compliant.
 
\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{images/reqs/reqs_distribution.pdf}
\caption{Labels distribution per requirement. Source: own elaboration.}
\label{fig:reqs_distribution}
\end{figure*}
 
\autoref{fig:reqs_correlation} shows the co-occurrence between non-compliant requirements. In other words, it measures the number of images with two different non-compliant requirements occurring together. As expected, there is a strong co-occurence between eyes-related characteristics, such as \lookingaway, \hairacrosseyes, \eyesclosed, \redeyes, \darktintedlenses, and \framecoveringeyes. This is mainly caused by the dummy requirements that were converted to non-compliant. Moreover, considerable co-occurrences can be observed between the requirements associated with skin, for instance, \unnaturalskintone and \flashskin. 
 
\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{images/reqs/reqs_correlation.pdf}
\caption{Co-occurrence between non-compliant requirements. The value indicates the number of images with both requirements non-compliant. The blue scale is normalized by row (the higher the darker). Source: own elaboration.}
\label{fig:reqs_correlation}
\end{figure*}
 
Based on our analysis, we arrived at the following conclusions. Firstly, since the dimensions of the images are different across the dataset, an approach to normalize images is required. It must avoid undesired normalization effects (like blur and pixelation) whenever possible and consider the trade-off between the input image quality and processing speed by the network. Regarding labels, the proposed method must consider dataset unbalancing, and the most unbalanced requirements may require special attention. Furthermore, the correlation between certain requirements can be leveraged by a mechanism that shares features, like Autoencoders.
 
In the next section, we describe in detail the proposed method, called \methodname. This includes the architecture, training process, and implementation details.
 
\subsection{\methodname}
 
\methodname is a \acl{dnn} developed to address the \icao standard. The architecture of \methodname is mainly based on Autoencoders. However, it has also been extended to apply a multi-and-collaborative learning approach. Further details regarding the proposed method are described in the remainder of this section. 
 
\subsubsection{Preprocessing} \label{sec:preprocessing}
 
Since the \adhoc dataset images have an extensive range of size dimensions (from 362$\times$362 to 4608$\times$3456 pixels), a preprocessing step was required to standardize the input image to \methodname. \autoref{fig:preprocessing} summarizes the preprocessing method applied in this work. A detailed explanation is provided below.
 
\begin{figure}[tb]
\centering
\includegraphics[width=\linewidth]{images/icaonet/preprocessing.pdf}
\caption{Preprocessing step of \methodname. Source: own elaboration.}
\label{fig:preprocessing}
\end{figure}
 
The first step in our preprocessing method is face detection. We use a single-shot multi-box detector based on MobileNet \citep{yeephycho} and trained on the WIDER FACE dataset \citep{yang2016wider}. This face detector was chosen because it balances speed and accuracy fairly. According to our benchmarks, 98.99 \% of all faces were detected, with an average processing time of 1.6s per image in the CPU. Moreover, this detector is compatible with TensorFlow 1.X versions used in the proposed method. 
 
Since the bounding box of the detected face is limited to the face region (from forehead to chin), we crop a squared region 1.5$\times$ larger than the detected face to include background and other relevant information to assess the requirements. The padded regions are filled with zeros because they generate fewer undesired artifacts than methods such as border reflection, replication, or wrapping. We do not apply any image normalization like illumination correction or rotation, as it can affect requirements evaluation. 
 
Finally, the cropped image is resized to 160$\times$160 pixels using the \texttt{INTER\_AREA} method of OpenCV\footnote{\url{https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html}}, since it is the recommended method for image decimation. Then, all pixel intensities are normalized to the real-valued $[0...1]$ range before being fed to \methodname. Again, the size of 160$\times$160 pixels was chosen, considering the trade-off between computational costs and results. More details are provided in Chapter \ref{sec:results}. 
 
\subsubsection{Architecture}
 
The overall architecture of \methodname can be seen in Figure \ref{fig:icaonet}. The architecture is composed of an Autoencoder combined with multiple dense network branches. While the Autoencoder is employed for unsupervised learning of a highly discriminative embedding space, the dense layers perform multi-label classification and landmark localization.
 
\begin{figure*}[tb]
\centering
\includegraphics[width=\linewidth]{images/icaonet/icaonet.pdf}
\caption{Architecture of \methodname. Source: own elaboration.}
\label{fig:icaonet}
\end{figure*}
 
The main idea behind the proposed architecture is to employ a multi and collaborative learning approach for \icao requirements. This is multi-learning (also called multitasking) because the network solves both regression (image reconstruction and landmark prediction), multi-label classification (compliance prediction for each requirement), and binary classification (pixelation assessment) tasks simultaneously. In contrast to \acfp{gan}, the learning process must also be collaborative since all tasks are optimized together in training time. Hence, all branches must collaborate to learn an appropriate representation for solving all tasks. Additionally, it can be possible to assign different weights for each task to determine their importance during the optimization step (more details are provided later).
 
The \methodname has three main components: (i) a \textbf{Shared Network} to compute shared embeddings, (ii) an \textbf{Unsupervised Branch} to perform image reconstruction, and (iii) \textbf{Supervised Branches} for requirement assessment and landmark localization. Such components are detailed as follows.
 
\paragraph{Shared Network}
 
The initial part of the network is responsible for learning the embeddings shared by unsupervised and supervised branches. These embeddings can be interpreted as a proper input representation and can be defined as an encoder function $h = f(x)$. Besides being used for dimensionality reduction, the shared embeddings are also used for feature learning in our network.
 
The architecture to compute the shared embeddings is based on the encoder component of Undercomplete Convolutional Autoencoder networks \citep[p.~500]{goodfellow2016deep}. The shared network receives a preprocessed input image of 160x160 pixels in the BGR color space. Details of the preprocessing are given in Section \ref{sec:preprocessing}. The first four convolutional layers are composed of 3x3 filters with batch normalization and ReLU activation. A 2D max-pooling layer is applied for dimensionality reduction after each layer. The fifth layer is also composed of 3x3 convolutions with batch normalization. However, $tanh$ is used as a non-linear activation function instead of ReLU to normalize the embedding values between -1 and 1. In our experiments, $tanh$ performed slightly better than ReLU for encoded representations. The output of this layer stores the embeddings shared by other branches. The embeddings are a 256-dimensional vector.
 
\paragraph{Unsupervised Branch}
 
This branch represents the decoder of an Autoencoder network. It is responsible for decoding the shared embeddings back to a lossy representation of the original input. Mathematically, this branch is responsible for learning the decoder function $\hat{x} = g(h)$, where $\hat{x}$ represents the reconstructed image. In backpropagation step, it also assists the creation of useful embeddings.
 
The architecture reflects the first four convolutional layers of the shared embeddings network. However, the Max-pooling layers are replaced with 2D Transposed Convolution layers. Furthermore, the last layer activation function is sigmoid since the input image pixels are normalized into the $[0...1]$ range. The shared embedding network and the unsupervised branch produce a fully Convolutional Autoencoder Network \citep{goodfellow2016deep}.
 
The unsupervised branch uses the Mean Squared Error (MSE) to measure the image reconstruction task ($\mathcal{L}_1$), as defined by \autoref{eq:loss-unsupervised}:
 
\begin{equation}
\label{eq:loss-unsupervised}
\mathcal{L}_1 = \frac{1}{N} \sum_h^H \sum_w^W ({I_{h,w} - \hat{I}_{h,w}})^2
\end{equation}
 
\noindent where $H$ and $W$ are the image dimensions; $N = H \times W$ represents the number of pixels; and $I$ and $\hat{I}$ are the input and the reconstructed images, respectively. Since the same input image $I$ is used in the reconstruction task as the ground-truth, this task is considered unsupervised (sometimes called semi-supervised). During training, the unsupervised branch tries to minimize the squared difference between the input image $I$ and the reconstructed image $\hat{I}$.
 
\paragraph{Supervised Branches} \label{sec:supervisedbranches}
 
There are three supervised branches in \methodname: (i) \textbf{requirements}, (ii) \textbf{ eye-landmark localization}, and (iii) \textbf{pixelation}. The requirements and landmark localization branches take the shared embeddings as input and apply a fully connected network to perform multi-label classification and regression, respectively. On the other hand, the pixelation branch leverages information from shallow layers to use low-level features in the evaluation of \pixelation requirement. 
 
The first layer of the requirement branch performs GlobalAveragePooling on the shared embeddings. It transforms the 4-D dimensional vector of the embeddings into the 2-D dimensions required by dense networks. We chose GlobalAveragePooling layers instead of MaxPooling layers because they are known to perform better in practice, as stated in \cite{zhou2016learning}. Then, there are two consecutive dense layers with 64 and 32 neurons, respectively. Dropout layers are included between each layer to prevent overfitting. Finally, the output layer contains 23 neurons with sigmoid activation. Therefore, each neuron of the final layer outputs a normalized score between 0 and 1 for each corresponding requirement. In practice, the supervised branch's output is considered the likelihood that a given input image is compliant with each requirement of the \icao standard.
 
The landmark localization branch is almost identical to the requirement branch. It also shares the GlobalAveragePooling, but the final layer has 4 neurons representing each eye's center $(x, y)$. The activation function of the output layer is also sigmoid since the eye positions are normalized to the input size. However, in this case, we are performing a regression task.
 
The pixelation branch also starts with GlobalAveragePooling applied after the second Max-Pooling layer of the Shared Network. Then, there are 3 consecutive layers of Dropout and fully connected layers with 64, 128, and 128 neurons, respectively. Finally, the last layer has a single neuron with sigmoid activation. It outputs the likelihood of the input image being compliant/non-compliant regarding the \pixelation requirement.
 
The pixelation branch deserves special considerations. First, unlike the other branches, it is the only branch that does not use the embeddings learned by the Shared Network as inputs. This branch begins after the second Max-Pooling layer of the Shared Network. Therefore, the high-level features present in these shallow layers are used to assess this specific requirement. Moreover, this is a particular case of \acl{mtl} since there is a task-specific head decoupled from the joint representation. Finally, the pixelation branch is trained with face patches of the original image, i.e., before preprocessing. In this way, the image resolution is preserved, and it avoids undesired pixelation effects artificially generated by the resize operation of preprocessing step. 
 
The Multi-label Cross-Entropy is used as the loss function for the requirements branch, defined by \autoref{eq:loss-supervised}:
 
\begin{equation}
\label{eq:loss-supervised}
\mathcal{L}_4 = -\frac{1}{M} \sum_i^M {y_i \cdot log(\hat{y}_i)}
\end{equation}
 
\noindent where $M$ represents the number of requirements; $y_i$ is the ground-truth for each requirement (0: non-compliant, 1: compliant); and $\hat{y}_i$ is the predicted score for each requirement.
 
The landmark localization branch employs the Wing loss ($\lambda_3$) as the loss function (see Section \ref{sec:wingloss}). The pixelation branch applies the regular Cross-Entropy loss ($\lambda_4$). Since the network has supervised and unsupervised branches, and they are optimized using different loss functions, the final loss function $\mathcal{L}(I, \hat{I}, y, \hat{y})$ is defined by \autoref{eq:loss-final}:
 
\begin{equation}
\label{eq:loss-final}
\mathcal{L}(I, \hat{I}, y, \hat{y}) = \lambda_1\mathcal{L}_1 + \lambda_2\mathcal{L}_2 + \lambda_3\mathcal{L}_3 + \lambda_4\mathcal{L}_4
\end{equation}
 
\noindent where $\lambda_{1...4}$ are hyperparameters to control the trade-off between each loss function. One important point to mention is that by using \autoref{eq:loss-final}, all branches will be optimized individually up to the the point where they are merged during backpropagation step. When it happens, the gradients of each loss function ($\mathcal{L}_{1...4}$ are summed. As discussed later in the subsection \ref{sec:hyperparams}, we experimentally tested different values of $\lambda_{1...4}$ via a grid-search approach. Nonetheless, more extensive searches are likely to yield more significant improvements, but these are left as future work.
 
\subsubsection{Training} 
 
To train \methodname, the \adhoc dataset was split into training and validation sets only. We consider the private dataset of the \acs{ficv} competition (\ficvofficial) as the test set, and the benchmark results are reported as our final results. Both the training and validation sets were randomly divided using a stratified multi-label approach. Thus, the compliant and non-compliant proportions listed in Table \ref{tab:req-dist} are preserved. Approximately 10\% of the dataset (580 images) was used as the validation set, and the remaining images were selected for network training.
 
The supervised branches of the \methodname were trained using distinct settings and timestamps. First, the unsupervised and requirement branches were trained together. Then, before training the pixelation and landmarks branches, the network was frozen to avoid changes in the requirement results when submitted to FVC. Later, the landmark localization branch was trained on a subset of the \adhoc dataset without the \darktintedlenses images. According to our experiments, removing these images could improve our predictions' performance because the landmarks labeled by the shape predictor of Dlib were incorrect in this case (see Section \ref{sec:databaseadhoc}). Finally, the pixelation branch was trained after the analysis to improve the results of this specific requirement. Such a branch was trained on approximately 9000 patches extracted from the original images (before preprocessing). Therefore, we avoided the resizing operation performed by the preprocessing step, which can create artificial pixelation artifacts.
 
Regarding the architecture, since most of the \methodname structure is based on undercomplete Autoencoders, there was no need to employ many regularization techniques during training. However, batch normalization was still applied before activation functions in the Convolutional layers and Dropouts in some dense layers of the supervised branches. The Early Stopping technique was also employed to prevent overfitting. All metrics described in Section \ref{sec:measures} of Chapter \ref{sec:background} are evaluated at the end of each epoch and will be reported in Chapter \ref{sec:results}.
 
The network is written in Python using Keras\footnote{\url{https://keras.io}} framework (v2.3.1) with TensorFlow (v1.13.1) backend. The Mlflow\footnote{\url{https://www.mlflow.org}} library (v1.7.0) is used for experiments tracking, comparison and logging of hyperparameters, metrics, and artifacts. The source code and the trained network can be found in Github\footnote{\url{https://github.com/arnaldog12/icaonet}}. Furthermore, the experiments were conducted on a Windows 10 machine with Intel\textsuperscript{\tiny\textregistered} Core\textsuperscript{\tiny\texttrademark} i5-8300H of 8\textsuperscript{th} generation, 16 GB of DDR4 2666 MHz RAM, SSD of 512 GB, and NVIDIA\textsuperscript{\tiny\textregistered} GeForce\textsuperscript{\tiny\textregistered} GTX 1050 with 4GB of RAM.
 
One last important detail about \methodname training is that special care was taken during experimentation. For example, the random seeds of Python, its \texttt{random} module\footnote{\url{https://docs.python.org/3/library/random.html}}, and Numpy\footnote{\url{https://numpy.org}} and TensorFlow libraries have been set to be the same for all experiments. It assures reproducibility and ensures that the best results of the current experiments are not achieved by chance.
 
\subsubsection{Parameters and Hyperparameters} \label{sec:hyperparams}
 
The base architecture of \methodname -- composed of the three main components described earlier (shared network, supervised branches, and unsupervised branch) -- contains 2,043,231 parameters. Of these parameters, 2,040,799 are trainable, and the remaining 2,432 are non-trainable parameters related to the mean and variance computed by batch norm layers. Regarding size, \methodname occupies 7.92 MB in the disk when stored as a \texttt{hdf5}\footnote{\url{https://www.hdfgroup.org/solutions/hdf5/}} file.
 
There are two important details to mention about the proposed architecture. The first refers to the unsupervised branch, which is used only during training. Once the network is trained, the unsupervised branch is detached from the model because the reconstruction task is not valuable for the ICAO assessment and can be ignored. In this case, the number of parameters is reduced to 1,063,068. Furthermore, after model \textit{freezing}\footnote{Freezing is a typical operation in Keras/TensorFlow models. It removes unnecessary data for prediction in the model file, for example, the optimizer, metrics, metadata, and gradients. It may not be confused with layer freezing.}, the model size is reduced to only 4.06 MB on the disk. It helps to speed up the running time considerably. Secondly, the score for \pixelation is outputted from both the pixelation and requirement branches during training. In fact, the pixelation output is kept in the requirements branch only for convenience. For the \acs{ficv} competition, only the score from the pixelation branch is considered.
 
The \methodname was trained using a batch size of 32. The pixelation branch was trained for 500 epochs, whereas the others were trained for 100. All layers were randomly initialized by Xavier initialization \citep{glorot2010understanding}. To prevent overfitting and improve generalization, we used Early Stopping with 30 epochs of patience for the requirement and landmark branches and 50 epochs for the pixelation branch. Also, the F-beta, \acs{mcc}, and Wing loss are used as monitoring metrics of the requirement, pixelation, and landmark localization branches, respectively. In layers where Dropout is applied, we keep 50\% of neurons. The Adaptive Momentum Estimation (Adam) is applied as the optimizer with learning rates $\alpha=10^{-3}$, $\beta_1=0.9$, $\beta_2=0.999$, and $\epsilon=10^{-7}$. For the final loss function (\autoref{eq:loss-final}), we used $\lambda_1=2.0$ (unsupervised), $\lambda_2=1.0$ (requirements), $\lambda_3=1.0$ (landmarks), and $\lambda_4=1.0$ (pixelation). Regarding eye location accuracy, the Wing loss (\autoref{eq:wingloss}) is applied with $w = 10$ and $\epsilon = 2$. All these parameters were chosen after a systematic search. However, it is conceivable that searching for better hyperparameters may further improve the performance of \methodname.
 
 
 