\section{Literature Review} \label{sec:literature}

In this chapter, we start by reviewing Deep \acl{mtl} techniques applied in computer vision problems. We focus on different types of Deep Multitask architectures and the most relevant works published for each category. Additionally, we provide a complete historical review of the methods that addressed the \icao standard, including the methods published in the \fvcongoing website \citep{fvcongoing}. Finally, we conclude this chapter by discussing relevant findings of the literature review of both topics.

\subsection{Representation Learning}

% A similar, but more difficult, problem that is closely related to \acs{mtl} is the \acf{ltl}, also known as inductive bias learning or transfer learning \citep{maurer2016benefit}. The main idea behind \acs{ltl} is to learn to perform a new task through the exploitation of knowledge acquired from previous tasks experiences. In other words, tasks can ``learn from each other'' by sharing parameters, for example. It has been proposed by \citep{thrun1998learning}, showing that a common representation which is able to perform well on training tasks will also generalize to new tasks retrieved from the same ``environment''. Recent papers has also shown dimension independent bounds \citep{maurer2013sparse, pontil2013excess, pentina2014pac}.

% A seminal line of research in the field of \acl{mtl} and Transfer Learning is centered around the concept that tasks are related by means of a shared low-dimensional representation that is jointly learned along with the task parameters \citep{maurer2016benefit}. This approach was first endorsed by the works of \cite{Caruana1997, thrun1998learning, baxter2000model} and later reexamined from a convex optimization and sparsity regularization perspective in \citep{argyriou2006multi}. Representation Learning is also a crucial aspect of \acl{ai}, leading to renewed interest in the development of nonlinear hierarchical representation through \aclp{dnn}. In the literature, there is a large body of works on Representation Learning, especially for the fields of \acf{nlp} \citep{Peters2018DeepCW, devlin2018bert, bojanowski2017enriching} and Computer Vision \citep{ren2015faster, krizhevsky2017imagenet}. Since then, the interest about \acf{mtrl} has increased as it is a core component of \acsp{dnn}.

According to the literature, we can split the Representation Learning approaches in two main categories: generative and discriminative modelling. Both approaches consider that a helpful representation can explain the variation in data by capturing the underlying factors. However, there are important differences in the modelling process, detailed as follows.

In generative approaches, the representation are learning by modelling the data distribution $p(x)$. In case of images, it can be the image pixels, for example. It presumes that if an useful model $p(x)$ can create realistic data samples, then it must also be able to capture the underlying structure associated with the dependent variable $y$. The conditional distribution $p(y|x)$ can be determined through Bayes' rule and allows the evaluation of discriminative tasks on $y$ \citep{le2020contrastive}. In general, generative approaches are categorized as unsupervised learning and can be seen, for instance, in the works of \cite{NIPS2014_5ca3e9b1, kingma2013auto}.

On the other hand, discriminative approaches learns a representation by directly modelling the conditional distribution $p(y|x)$ through a parametric model. This model takes the data sample $x$ as input and outputs the label variable $y$. Latent variables $p(v|x)$ are inferred and downstream decisions $p(y|v)$ are made from those variables. Most discriminative approaches are a type of supervised learning (sometimes called ``self-supervised''). Examples of this approach can be found in \citep{dosovitskiy2014discriminative, Zhang_2017_CVPR}.

In comparison to generative approach, the discriminative models present some advantages. Primarily, modeling the distribution of $x$ is computationally intensive and not essential for extracting representations. Furthermore, the generation process of a generative model can be deemed inefficient if the objective is to only obtain the lower dimensional representation. Lastly, generative models usually employ more expensive objective function especially designed for the input space.

There are many fields and applications that Representation Learning plays an important role. First, we can highlight the field of \acl{nlp}. It all started with the idea of distributed representations for symbolic data introduced by \cite{hinton1986learning}, which was later implemented by \cite{bengio2000neural} in the context of statistical language modeling. In \citep{collobert2011natural}, the authors developed SENNA, a convolutional architecture to share representations across different \acs{nlp} tasks, including: part-of-speech tagging, named entity recognition, chunking, semantic role labeling, language modeling, and syntactic parsing. Methods for learning word representations can also be seen in the works of \cite{mnih2013learning, pennington2014glove}. Usually, they are based on unsupervised objectives for predictions of words or word frequencies from raw text. These methods have demonstrated remarkable achievements when used for transfer learning, overcoming supervised models across multiple downstream tasks.

In Computer Vision, nowadays the most popular methods for image feature learning are based on \aclp{cnn}. Traditional architecture like AlexNet \citep{krizhevsky2017imagenet}, VGG \citep{simonyan2014very}, and ResNet \citep{he2016deep} are examples of models used for this purpose. They extract hierarchical features learned from input images and that can be later applied to other classifiers, like Neural Networks or SVMs. Additionally, previously mentioned encoder-decoder architectures like Autoencoders are also a famous method of unsupervised learning for representation learning. Examples of other Computer Vision models and architectures will be detailed in the next section.

\subsection{Multitask Learning}

\acl{mtl} can be defined as a technique to learn multiple tasks \textit{jointly} instead of learning each task independently. It has been studied since the 90's, initially focused on Neural Network models \citep{ thrun1998learning, Caruana1997, baxter2000model}. More recent approaches have been based on structured sparsity and convex optimization \citep{argyriou2006multi}, kernel methods \citep{evgeniou2005learning}, among others.

Historically, \acs{mtl} methods were classified into hard or soft parameter sharing techniques \citep{vandenhende2021multi}. In the hard parameter sharing, initially proposed by \cite{Caruana1997}, the network parameters are divided into shared and task-specific parameters. Typically, \acs{mtl} models using hard parameter sharing contain a shared encoder with task-specific branches \citep{kendall2018multi, chen2018gradnorm, sener2018multi}. As proved by \cite{baxter1997bayesian}, the risk of overfitting in hard parameter sharing is order $N$ smaller than overfitting the task-specific branches, where $N$ is the number of tasks. Intuitively, since it is harder for a model to find a shared representation for all tasks, the less likely is the chance of overfitting. On the other hand, in soft parameter sharing, the parameters of each task are handled by a feature sharing mechanism \citep{ruder2019latent, gao2019nddr, liu2019end}. In this case, each task has a specific model, and the parameters of each model are regularized to be similar. \autoref{fig:mtl_sharing} visually explains the differences between hard and soft parameters sharing.

\begin{figure*}[ht]
\centering
\subfigure[Hard Parameter Sharing]{\includegraphics[height=0.35\linewidth]{images/multitask_learning/mtl_hard.pdf}}
\hfill
\subfigure[Soft Parameter Sharing]{\includegraphics[height=0.35\linewidth]{images/multitask_learning/mtl_soft.pdf}}    
\caption{Types of parameter sharing in \acl{mtl}. The network backbone is represented in blue, while the task-specific heads are indicated in green. Source: own elaboration.}
\label{fig:mtl_sharing}
\end{figure*}

A variety of techniques and architectures for \acs{mtl} has been proposed in the literature of Deep Learning. Usually, Deep Multitask Architectures may be divided into encoder-focused and decoder-focused architectures \citep{vandenhende2021multi}. The main characteristic of the encoder-focused architectures \citep{kendall2018multi, chen2018gradnorm, sener2018multi} is the presence of an off-the-shelf backbone network, usually called an encoder. The goal of the encoder is to learn a generic representation that will be shared by a set of independent task-specific heads. Differently, the decoder-focused architectures also exchange information during the decoding stage \citep{xu2018pad, zhang2018joint, vandenhende2020mti}. In the following subsections, we discuss the most relevant Deep \acl{mtl} architectures.

\subsubsection{Encoder-focused Architectures}

The \textbf{Cross-stitch networks} \citep{misra2016cross} combines two given activation maps $x_A$ and $x_B$ - that belongs to tasks $A$ and $B$ respectively -, in a learnable linear way. The transformation can be expressed by learnable weights $\alpha$, as shown in the \autoref{eq:cross-stitch}:

\begin{equation}
\label{eq:cross-stitch}
\begin{bmatrix}
\bar{x}_A\\ 
\bar{x}_B
\end{bmatrix} = 
\begin{bmatrix}
\alpha_{AA} & \alpha_{AB} \\ 
\alpha_{BA} & \alpha_{BB}
\end{bmatrix}
\begin{bmatrix}
x_A \\ 
x_B
\end{bmatrix}
\end{equation}

The $\alpha$ for each task are initialized in the range $[0, 1]$ and obtained during training through a convex combination. By using this equation, the Cross-stitch networks can decide the degree to which the features are shared between different tasks. However, to maximize performance, such networks must be pre-trained before stitching them together. Additionally, the size of the Cross-stitch network linearly increases with the number of tasks. 

\textbf{Neural Discriminative Dimensionality Reduction CNNs} (NDDR-CNNs) \citep{gao2019nddr} presents a similar architecture with Cross-stitch networks. Nonetheless, a dimensionality reduction component is employed instead of the linear combination to merge all single-task networks' activations. However, besides the NDDR-CNNs being susceptible to the same problems of the Cross-stitch networks, they also require additional design choices (e.g., where to include the NDDR layers). Nevertheless, both NDDR-CNNs and Cross-stitch are limited to local information when the activations from different single-task networks are fused.

The \textbf{Multitask Attention Networks} (MTAN) \citep{liu2019end} are an encoder-focused design that combines the encoder with task-specific attention modules in the backbone network. While the encoder is responsible for computing a general pool of features, the task-specific attention module chooses features from the general pool by applying a soft attention mask. Regular convolutional layers with sigmoids are used to implement the attention mechanism. Compared to Cross-stitch networks and NDDR-CNNs, the MTAN model is less prone to scalability issues but is also limited to local information to produce the attention mask.

\subsubsection{Decoder-focused Architectures}

One of the first decoder-focused architectures published in the literature was \textbf{PAD-Net} \citep{xu2018pad}. Although the input image is still performed by an off-the-shelf backbone network, the backbone features are further processed by task-specific heads that produce initial predictions for each task. The task-specific heads contain a per-task feature representation of the input image and are recombined by a multi-modal distillation. The main goal of the distillation unit is to extract cross-task information by using a spatial attention mechanism. The output features $F_k^o$ for a given task $k$ and a training sample $i$ are computed by the \autoref{eq:padnet}:

\begin{equation}
\label{eq:padnet}
F_k^o = F_k^i + \sum {\sigma (W_{k,l}F_l^i) \odot F_l^i}
\end{equation}

\noindent where $l$ is the feature map index, $W_{k,l}$ is the convolution parameter, $\sigma$ represents a sigmoid function, and $\odot$ denotes element-wise multiplication. However, \autoref{eq:padnet} presumes that task interactions are location independent. Therefore, there must be no relationship between tasks across the entire image.

Similar to PAD-Net, the \textbf{Pattern-Affinitive Propagation Networks} (PAP-Net) \citep{zhang2019pattern}, improved the multi-model distillation. By a statistical observation that pixel affinities contribute to a better alignment with common local structures on the task label space, they proposed to leverage pixel affinities to perform multi-modal distillation. A pixel affinity matrix $M_{T_j}$ is computed by estimating pixel-wise correlations upon the task features coming from each task-specific head. Then, a cross-task information matrix $\hat{M}_{T_j}$ for each task $T_j$ is learned by an adaptive combination of the affinity matrices $M_{T_j}$ for tasks $T_i$ with learnable weights $\alpha_i^{T_j}$, as defined in \autoref{eq:papnet}:

\begin{equation}
\label{eq:papnet}
\hat{M}_{T_j} = \sum_{T_i} {\alpha_i^{T_j} \cdot M_{T_i}}
\end{equation}

\noindent
where $i$ represents the index of other tasks. The task features of a task $j$ are refined by the cross-task information matrix $M_{T_j}$. In fact, $M_{T_j}$ is dissipated across the task features space to spread the pixel correlation for task $T_j$ based on the pixel similarities from the other tasks $T_i$. The learnable weights $\alpha_i^{T_j}$ are obtained by affinity learning layers at the decoding process with different input scales. Unlike the other decoder-focused architectures mentioned, the PAP-Net also models the non-local relationships through pixel similarities computed across the whole image.

The \textbf{Joint Task-Recursive Learning} (JTRL), proposed by \cite{zhang2018joint}, recursively predicts two tasks by increasing higher scales to refine the results of past states gradually. Compared to PAD-Net and PAP-Net, there is also a multi-modal mechanism that combines information from earlier task predictions, which are used to refine the later ones. However, the JTRL model is only able to predict two tasks sequentially and in an intertwined approach. Moreover, the main drawback of the JTRL model is that it is not simple, or even possible, to extend the architecture to more than two tasks because of the intertwined approach to refine predictions.

In this work, an encoder-focused architecture is applied since there are requirements in the \icao standard that shares similar characteristics (e.g., requirements related to the eyes or background). Therefore, the generic representation learned by the encoder might be useful when shared with the task-specific heads. Compared to the encoder-focused architectures cited in this paper, the proposed method does not require pre-training like the Cross-stitch networks or advanced design choices as in NDDR-CNNs. In addition, the \methodname is easy to scale like MTAN networks. More details will be discussed in Chapter \ref{sec:method}.

\subsection{Methods for the \icao Standard}

One of the first studies to address the ICAO requirements was proposed by \citet{sang2009face}. It presents methods to evaluate requirements related to illumination conditions and facial pose based on Gabor wavelet features. Furthermore, a method to evaluate the image blur is proposed through the Discrete Cosine Transform (DCT). The authors assess their methods using images from CMU-PIE and FERET datasets. However, only analytical results are presented.

The popularization of methods for \icao standard can be credited to the Biolab group from the University of Bologna. In 2009, they presented the Biolab-ICAO framework \citep{maltoni2009biolab}, a benchmark tool for systems assessing face image compliance to ICAO requirements. In 2012, the benchmark was refined, and the official ground truth face database (4868 images) and testing protocol were presented \citep{ferrara2012face}. In summary, the dataset has 5588 images collected from different sources or artificially generated. A subset of 720 images are publicly provided for the participants, while the 4468 remaining image are private and used for the evaluation of submitted algorithms. The full description of such dataset is provided in Section \ref{sec:database}. Moreover, the authors proposed the BioLabSDK, the first known method published in the literature able to evaluate all the 23 face-and-pose requirements (8--30 in Table \ref{tab:icao}). The BiolabSDK uses different color spaces, face detection, and points that define the face and its elements to generate a score for each requirement. The paper also compares the BioLabSDK against two anonymous SDKs using the \acs{eer} and Rejection Rates. The results can be seen in the first three columns of \autoref{tab:comp}.

\input{tables/table_comp}

Today, the Biolab-ICAO framework is used to evaluate algorithms via an online public competition called Face Image ISO Compliance Verification (FICV), hosted at the \fvcongoing website \citep{fvcongoing}. The FICV is considered the official evaluation tool for \icao standard and is used by all relevant works presented in the literature or commercial products. The photographic and pose requirements (\#8--\#30) are evaluated individually in terms of EER. All the results presented in Table \ref{tab:comp} and the rest of this paper were evaluated by the FICV.

To date, there are four published algorithms in the \fvcongoing platform: BioTest \citep{fvcBioTest}, BioPass Face \citep{fvcVsoft}, id3 \citep{fvcICAOCompliance}, and ICAO SDK \citep{fvcSeamfix} (see Table \ref{tab:comp}). In comparison with the BioLabSDK, such algorithms achieve comparable or even better performance rates in some requirements. However, all of these algorithms are commercial \citep{biometrika, id3, seamfix, vsoft}. Thus, there is no detailed explanation of their methodology in the scientific literature.

\cite{ferrara2012multi} present a segmentation method for passport images based on a multi-classifier approach. Using position, color, texture, and histogram classifiers, the algorithm proposed by the authors classifies and post-processes regions in the face image to segment them into four distinct classes: face, hair, clothes, and background. However, the authors apply the segmentation result to analyze only three ICAO requirements (\hairacrosseyes, \variedbackground, and \flashskin), obtaining EERs of 13.87\%, 6.35\%, and 0.77\%, respectively. We denominated this method as ``FerraraSeg'' in Table \ref{tab:comp}. There are two other face segmentation works for passport images in the literature: \cite{hirzer2009automatic} and \cite{subasic2009expert}. Nonetheless, they do not analyze their results regarding the \icao standard.

The work of \citet{nguyen2013automated} proposes a set of normalized metrics for quantitative conformance testing of the ICAO requirements. Their method has three main steps: foreground/background segmentation, face detection, and facial feature extraction. Each step takes advantage of color, intensity, and edge information to compute scores for a subset of requirements. The metrics are evaluated over a subset of FERET \citep{phillips1998feret}, GTAV \citep{tarres2012gtav}, and FIePI databases. However, results about EER are not presented, which is a common practice of algorithms that assess the \icao requirements in the literature.

In \cite{parente2016assessing}, methods for individual evaluation of four requirements were proposed. For the \pixelation requirement, the Canny edge detection method is applied in the eye region, and the Hough Transform is used to detect horizontal and vertical lines. In the case of \hairacrosseyes, both eye regions are preprocessed with classic techniques and compared via an XOR operator. For \veiloverface, the authors compute a score based on the proportion of skin pixels presented in the lower region of the face using the YCrCb color space. Finally, to assess the \mouthopen requirement, the method analyzes teeth and lips based on a color search approach inside the detected mouth. The results for each requirement can be seen in Table \ref{tab:comp}.

The requirements \unnaturalskintone, \shadowsacrossface, and \flashskin are evaluated in the work of \citet{andrezza2016facial}. Since these requirements are directly related to skin, the authors developed a custom segmentation method. Each pixel in the face image that falls into predefined ranges of YCrCb color space is marked as skin. To evaluate the \unnaturalskintone, a score is computed based on the proportion of pixels with natural tone according to histogram analysis. For \flashskin, a metric is defined based on the binarized image of the Y channel of the YCrCB. Similarly, the Z channel of XYZ color space is analyzed to evaluate shadows in the face. In Table \ref{tab:comp}, the results are shown in the column named ``Andrezza et al.''.

The work of \citet{borges2016analysis} analyses some of the requirements related to eyes: \eyesclosed, \redeyes, and \lookingaway. First, an appearance-based method is applied to find the eye corners and estimate the iris center based on the Canny edge detector and Hough Circles Transform. Such information is used by the remaining methods. To detect if the eyes are closed or opened, the authors compute a metric based on the eye dimensions and the presence of sclera. For the evaluation of \redeyes, custom computations are performed on RGB, HSV, and YCrCB to find the ``red'' pixels and three corresponding binarized images are generated. A score is then computed based on logical operations performed in the combination of these images. Finally, to assess the \lookingaway requirement, the authors assume the eyes are symmetric and inspect both left and right sides of each eye. An OR operation is applied between both sides, and a score is computed based on the proportion of the minimum and maximum sums of white pixels. This method is identified by ``Borges et al.'' in Table \ref{tab:comp}.

One of the first works that employ a \acl{dl}-based method for \icao was presented by \cite{ahmadvand2018estimating}. The authors apply the fine-tuning technique in the VGGFace model \citep{simonyan2014very} to train a new model that assesses the \rollpitchyaw requirement. Six different datasets are employed with a total of 320,000 images, where only 12,000 are compliant with the ICAO standard. The cross-entropy is used to optimize the model, and the accuracy was chosen to evaluate the final results. The authors reports 95.5\% and 97.8\% of accuracy in the PIE \citep{sim2002cmu} and CSIE Robotic \citep{csie2006database} databases, respectively. Evaluation results according to the FICV competition are not mentioned in the paper.

The \biolabicao framework is used by \cite{hernandez2019faceqnet} to train a method based on \aclp{cnn}, called FaceQnet. In this case, the framework is applied for labeling the VGGFace2 dataset \citep{cao2018vggface2}. The ResNet-50 \citep{he2016deep} is fine-tuned to return a score that represents a numerical quality measure for each input image. The authors analyze if the score can determine whether an image can be suitable for face recognition. However, the authors do not provide results regarding the FICV competition.

Finally, the most recent method to evaluate ICAO requirements was published in 2019 \citep{nourbakhshfacial}. The authors propose a method based on the Hierarchical Max-pooling (HMAX) model, consisting of a CNN with multi-resolution spatial pooling. First, face components are extracted from image patches using the Viola-Jones algorithm \citep{viola2001rapid}. Then, the HMAX model is applied to acquire discriminative signatures. The AR \citep{martinez1998ar} and PUT \citep{kasinski2008put} databases are used to train the model in 9 requirements. In Table \ref{tab:comp}, their results are represented by ``HMAX'' column.

\subsection{Conclusions}

By analyzing the literature of \acl{mtl}, we can highlight some relevant points. First, the \acs{mtl} by itself is a relatively new study field. Thus, most of the research about this topic is still beginning, and some gaps can be filled (e.g., the high \acs{eer} of some requirements or the lack of representative datasets). Secondly, the encoder/decoder-focused architectures present their advantages and drawbacks. For example, in encoder-focused architectures, the generic representation learned by the encoder might be valuable when shared with the task-specific heads. However, they may fail to capture familiar and different aspects among tasks. On the other hand, decoder-focused architectures also share or exchange information during the decoding stage. It can help improve the performance, but usually, such networks assume independent tasks or are limited to the number of tasks they can solve. 

In this thesis proposal, an encoder-focused architecture is proposed since there are requirements in the \icao standard that shares similar characteristics (e.g., requirements related to the eyes or background). Therefore, the features learned by the encoder for some requirements can be leveraged to the task-specific heads.  Compared to the encoder-focused architectures cited in this chapter, the proposed method does not require pre-training like the Cross-stitch networks or advanced design choices as in NDDR-CNNs. In addition, the \methodname is easy to scale like MTAN networks. More details will be discussed in Chapter \ref{sec:method}.

Regarding Representation Learning, our work employs both generative and discriminative approaches. The generative approach is present in the unsupervised component responsible for creating realistic data samples while it learns an useful representation (called embeddings) by modeling the input distribution. On the other hand, the same representation is also applied to predict different outputs (i.e., requirements and landmark localization). It allows the creation of an useful shared representation built from errors back-propagated from related tasks, as stated in \citep{zhang2014facial}. Also, a more robust requirements assessment can be achieved through joint learning with heterogeneous but subtly correlated tasks. Again, a detailed explanation of our architecture will be provided in Chapter \ref{sec:method}.

Concerning the literature over the \icao standard, we can conclude that, although many works have been addressing this problem for more than a decade, it is still an open challenge. For instance, there are still requirements with EERs greater than 10\% as the best result among all published works (e.g., \lookingaway and \hairacrosseyes). Another point is the fact that the majority of best results for each requirement presented in \autoref{tab:comp} are dominated by private companies, such as \cite{biometrika}, \cite{id3}, \cite{seamfix}, and \cite{vsoft}. Therefore, there is no detailed explanation about their methods, and it shows the lack of state-of-art methods published as open research. In fact, 12 out of the best results for all the 23 requirements are owned by private companies. Another gap is the low amount of single methods that evaluate all requirements. Only three methods (\biolab, \biotest, \biopass) can evaluate all of them. We believe this is due to the absence of public datasets specialized for the ICAO problem. Finally, we can also conclude that \acl{dl} can be a helpful approach to improve the current results of methods that address the ICAO requirements. One example is the HMAX work, which achieved 0.0\% of EER in two requirements - \framestooheavy and \framecoveringeyes - with very low rejection rates.
