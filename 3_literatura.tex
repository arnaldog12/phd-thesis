\section{Literature Review} \label{sec:literature}

In this chapter, we start by reviewing deep \acl{mtl} techniques applied in computer vision problems. We focus on different types of deep Multitask architectures and the most famous works published for each category. Additionally, we provide a complete historical review of the methods that addressed the \icao standard, including the methods published in the \fvcongoing website. Finally, we conclude this chapter by discussing relevant findings of the literature review of both topics.

\subsection{Multitask Learning}

A variety of techniques and architectures for \acs{mtl} has been proposed in the literature of Deep Learning. Usually, Deep Multitask Architectures may be divided into encoder-focused and decoded-focused architectures \citep{vandenhende2021multi}. The main characteristic of the encoder-focused architectures \citep{kendall2018multi, chen2018gradnorm, sener2018multi} is the presence of an off-the-shelf backbone network, usually called an encoder. The goal of the encoder is to learn a generic representation that will be shared by a set of independent task-specific heads. Differently, the decoder-focused architectures also exchange information during the decoding stage \citep{xu2018pad, zhang2018joint, vandenhende2020mti}. In the following subsections, we discuss the most relevant Deep \acl{mtl} architectures.

\subsubsection{Encoder-focused Architectures}

The \textbf{Cross-stitch networks} \citep{misra2016cross} combines two given activation maps $x_A$ and $x_B$ - that belongs to tasks $A$ and $B$ respectively -, in a learnable linear way. The transformation can be expressed by learnable weights $\alpha$, as shown in the \autoref{eq:cross-stitch}:

\begin{equation}
\label{eq:cross-stitch}
\begin{bmatrix}
\bar{x}_A\\ 
\bar{x}_B
\end{bmatrix} = 
\begin{bmatrix}
\alpha_{AA} & \alpha_{AB} \\ 
\alpha_{BA} & \alpha_{BB}
\end{bmatrix}
\begin{bmatrix}
x_A \\ 
x_B
\end{bmatrix}
\end{equation}

By using this equation, the Cross-stitch networks can decide the degree to which the features are shared between different tasks. However, to maximize performance, such networks must be pre-trained before stitching them together. Additionally, the size of the cross-stitch network linearly increases with the number of tasks. 

\textbf{Neural Discriminative Dimensionality Reduction CNNs} (NDDR-CNNs) \citep{gao2019nddr} presents a similar architecture with cross-stitch networks. Nonetheless, a dimensionality reduction component is employed instead of the linear combination to merge all single-task networks' activations. However, besides the NDDR-CNNs being susceptible to the same problems, they also require additional design choices (e.g., where to include the NDDR layers). Nevertheless, both networks are limited to local information when the activations from different single-task networks are fused.

The \textbf{Multitask Attention Networks} (MTAN) \citep{liu2019end} are a encoder-focused design that combine the encoder with task-specific attention modules in the backbone network. While the encoder is responsible for computing a general pool of features, the task-specific attention module chooses features from the general pool by applying a soft attention mask. Regular convolutional layers with sigmoids are used to implement the attention mechanism. Compared to cross-stitch networks and NDDR-CNNs, the MTAN model is also limited to local information to produce the attention mask but is less prone to scalability issues.

\subsubsection{Decoder-focused architectures}

One of the first decoder-focused architectures published in the literature was \textbf{PAD-Net} \citep{xu2018pad}. Although the input image is still handled by an off-the-shelf backbone network, the backbone features are further processed by task-specific heads that produce initial predictions for each task. The task-specific heads contain a per-task feature representation of the input image and are recombined by a multi-modal distillation. The main goal of the distillation unit is to extract cross-task information by using a spatial attention mechanism. The output features $F_k^o$ for a given task $k$ are computed by the \autoref{eq:padnet}:

\begin{equation}
\label{eq:padnet}
F_k^o = F_k^i + \sum {\sigma (W_{k,l}F_l^i) \odot F_l^i}
\end{equation}

\noindent where $\sigma (W_{k,l}F_l^i)$ represents the spatial attention mask applied to the initial task features $F_l^i$ from task $l$. However, \autoref{eq:padnet} presumes that task interactions are location independent. Therefore, there must be no relationship between tasks across the entire image.

Similar to PAD-Net, the \textbf{Pattern-Affinitive Propagation Networks} (PAP-Net) \citep{zhang2019pattern}, improved the multi-model distillation. By a statistical observation that pixel affinities contribute to a better alignment with common local structures on the task label space, they proposed to leverage pixel affinities to perform multi-modal distillation. A pixel affinity matrix $M_{T_j}$ is computed by estimating pixel-wise correlations upon the task features coming from each task-specific head. Then, a cross-task information matrix $\hat{M}_{T_j}$ for each task $T_j$ is learned by an adaptive combination of the affinity matrices $M_{T_j}$ for tasks $T_i$ with learnable weights $\alpha_i^{T_j}$, as defined in \autoref{eq:papnet}:

\begin{equation}
\label{eq:papnet}
\hat{M}_{T_j} = \sum_{T_i} {\alpha_i^{T_j} \cdot M_{T_i}}
\end{equation}

The task features of a task $j$ are refined by the cross-task information matrix $M_{T_j}$, which is dissipated across the task features space to spread the pixel correlation for task $T_j$ based on the pixel similarities from the other tasks $T_i$. Unlike the other decoder-focused architectures mentioned, the PAP-Net also models the non-local relationships through pixel similarities computed across the whole image.

The \textbf{Joint Task-Recursive Learning} (JTRL), proposed by \cite{zhang2018joint}, recursively predicts two tasks by increasing higher scales to refine the results of past states gradually. Compared to PAD-Net and PAP-Net, there is also a multi-modal mechanism that combines information from earlier task predictions, which are used to refine the later ones. However, the JTRL model is only able to predict two tasks sequentially and in an intertwined approach. Moreover, the main drawback of the JTRL model is that it is not simple, or even possible, to extend the architecture to more than two tasks because of the intertwined approach to refine predictions.

\subsection{Methods for the \icao standard}

One of the first studies to address the ICAO requirements was proposed by \citet{sang2009face}. It presents methods to evaluate requirements related to illumination conditions and facial pose based on Gabor wavelet features. Furthermore, a method to evaluate the image blur is proposed through the Discrete Cosine Transform (DCT). The authors assess their methods using images from CMU-PIE and FERET datasets. However, only analytical results are presented.

The popularization of methods for \icao standard can be credited to the Biolab group from the University of Bologna. In 2009, they presented the Biolab-ICAO framework \citep{maltoni2009biolab}, a benchmark tool for systems assessing face image compliance to ICAO requirements. In 2012, the benchmark was refined, and the official ground truth face database (4868 images) and testing protocol were presented \citep{ferrara2012face}. Moreover, the authors proposed the BioLabSDK, the first known method published in the literature able to evaluate all the 23 face-and-pose requirements (8--30 in Table \ref{tab:icao}). The BiolabSDK uses different color spaces, face detection, and points that define the face and its elements to generate a score for each requirement. The paper also compares the BioLabSDK against two anonymous SDKs using the Equal Error Rate (EER). The results can be seen in the first three columns of Table \ref{tab:comp}.

\input{tables/table_comp}

Today, the Biolab-ICAO framework is used to evaluate algorithms via an online public competition called Face Image ISO Compliance Verification (FICV), hosted at the \fvcongoing website \citep{fvcongoing}. The FICV is considered the official evaluation tool for \icao standard and is used by all relevant works presented in the literature or commercial products. All the results presented in Table \ref{tab:comp} and the rest of this paper were evaluated by the FICV.

To date, there are four published algorithms in the \fvcongoing platform: BioTest \citep{fvcBioTest}, BioPass Face \citep{fvcVsoft}, id3 \citep{fvcICAOCompliance}, and ICAO SDK \citep{fvcSeamfix} (see Table \ref{tab:comp}). In comparison with the BioLabSDK, some algorithms achieve comparable or even better performance rates in some requirements. However, all of these algorithms are commercial. Thus, there is no detailed explanation of their methodology in the scientific literature.

The method proposed in \cite{ferrara2012multi} presents a segmentation method for passport images based on a multi-classifier approach. Using position, color, texture, and histogram classifiers, the algorithm proposed by the authors classifies and post-processes regions in the face image to segment them into four distinct classes: face, hair, clothes, and background. However, the authors apply the segmentation result to analyze only 3 ICAO requirements, obtaining EERs of 13.87\%, 6.35\%, and 0.77\%. We denominated this method as ``FerraraSeg'' in Table \ref{tab:comp}. There are two other face segmentation works for passport images in the literature: \cite{hirzer2009automatic} and \cite{subasic2009expert}. Nonetheless, they do not analyze their results regarding the \icao standard.

The work of \citet{nguyen2013automated} proposes a set of normalized metrics for quantitative conformance testing of the ICAO requirements. Their method has three main steps: foreground/background segmentation, face detection, and facial feature extraction. Each step takes advantage of color, intensity, and edge information to compute scores for a subset of requirements. The metrics are evaluated over a subset of FERET \citep{phillips1998feret}, GTAV \citep{tarres2012gtav}, and FIePI databases. However, results about EER are not presented, which is a common practice of algorithms that assess the \icao requirements in the literature.

In \cite{parente2016assessing}, methods for individual evaluation of four requirements were proposed. For the \citeReq{\pixelation} requirement, the Canny edge detection method is applied in the eye region, and the Hough Transform is used to detect both horizontal and vertical lines. In the case of \citeReq{\hairacrosseyes}, both eye regions are preprocessed with classic techniques and compared via an XOR operator. For \citeReq{\veiloverface}, the authors compute a score based on the proportion of skin pixels presented in the lower region of the face using the YCrCb color space. Finally, to assess the \citeReq{\mouthopen} requirement, the method analyzes teeth and lips based on a color search approach inside the detected mouth. The results for each requirement can be seen in Table \ref{tab:comp}.

The requirements \citeReq{\unnaturalskintone}, \citeReq{\shadowsacrossface}, and \citeReq{\flashskin} are evaluated in the work of \citet{andrezza2016facial}. Since these requirements are directly related to skin, the authors developed a custom segmentation method. Each pixel in the face image that falls into predefined ranges of YCrCb color space is marked as skin. To evaluate the \citeReq{\unnaturalskintone}, a score is computed based on the proportion of pixels with natural tone according to histogram analysis. For \citeReq{\flashskin}, a metric is defined based on the binarized image of the Y channel of the YCrCB. Similarly, the Z channel of XYZ color space is analyzed to evaluate shadows in the face. In Table \ref{tab:comp}, the results are shown in the column named ``Andrezza et al.''.

The work of \citet{borges2016analysis} analyses some of the requirements related to eyes: \citeReq{\eyesclosed}, \citeReq{\redeyes}, and \citeReq{\lookingaway}. First, an appearance-based method is applied to find the eye corners and estimate the iris center based on the Canny edge detector and Hough Circles Transform. Such information is used by the remaining methods. To detect if the eyes are closed or opened, the authors compute a metric based on the eye dimensions and the presence of the sclera. For the evaluation of \citeReq{\redeyes}, custom computations are performed on RGB, HSV, and YCrCB to find the ``red'' pixels, and three corresponding binarized images are generated. A score is then computed based on logical operations performed in the combination of these images. Finally, to assess the \citeReq{\lookingaway} requirement, the authors assume the eyes are symmetric and inspect both left and right sides of each eye. An OR operation is applied between both sides, and a score is computed based on the proportion of the minimum and maximum sums of white pixels. This method is identified by ``Borges et al.'' in Table \ref{tab:comp}.

One of the first works that employ a \acl{dl}-based method for \icao was presented by \cite{ahmadvand2018estimating}. The authors apply the fine-tuning technique in the VGGFace model \citep{simonyan2014very} to train a new model that assesses the \citeReq{\rollpitchyaw} requirement. Six different datasets are employed with a total of 320,000 images, where only 12,000 are compliant with the ICAO standard. The cross-entropy is used to optimize the model, and the accuracy was chosen to evaluate the final results. The authors reports 95.5\% and 97.8\% of accuracy in the PIE \citep{sim2002cmu} and CSIE Robotic \citep{csie2006database} databases, respectively. Evaluation results according to the FICV competition are not mentioned in the paper.

The \biolabicao framework is used by \cite{hernandez2019faceqnet} to train a method based on \aclp{cnn}, called FaceQnet. In this case, the framework is applied for labeling the VGGFace2 dataset \citep{cao2018vggface2}. The ResNet-50 \citep{he2016deep} is fine-tuned to return a score that represents a numerical quality measure for each input image. The authors analyze if the score can determine whether an image can be suitable for face recognition. However, the authors do not provide results regarding the FICV competition.

Finally, the most recent method to evaluate ICAO requirements was published in 2020 \citep{nourbakhshfacial}. The authors propose a method based on the Hierarchical Max-pooling (HMAX) model, which consists of a CNN with multi-resolution spatial pooling. First, face components are extracted from image patches using the Viola-Jones algorithm \citep{viola2001rapid}. Then, the HMAX model is applied to acquire discriminative signatures. The AR \citep{martinez1998ar} and PUT \citep{kasinski2008put} databases are used to train and test the model in 9 requirements. In Table \ref{tab:comp}, their results are represented by ``HMAX'' column.

\subsection{Conclusions}

By analyzing the literature of \acl{mtl}, we can highlight some points. First, the \acs{mtl} by itself is a relatively new study field. Thus, most of the research about this topic is still beginning, and some gaps can be filled. Secondly, the encoder/decoder-focused architectures present their advantages and drawbacks. For example, in encoder-focused architectures, the generic representation learned by the encoder might be valuable when shared with the task-specific heads. However, they may fail to capture familiar and different aspects among tasks. On the other hand, decoder-focused architectures also share or exchange information during the decoding stage. It can help improve the performance, but usually, such networks assume independent tasks or are limited to the number of tasks they can solve. 

In this work, an encoder-focused architecture is applied since there are requirements in the \icao standard that shares similar characteristics (e.g., requirements related to the eyes or background). Therefore, the features learned by the encoder for some requirements can be leveraged to the task-specific heads. Compared to the encoder-focused architectures cited in this chapter, the proposed method does not require pre-training like the Cross-stitch networks or advanced design choices as in NDDR-CNNs. In addition, the \methodname is easy to scale like MTAN networks. More details will be discussed in chapter \ref{sec:method}.

Concerning the literature over the \icao standard, we can conclude that, although many works have been addressing this problem for more than a decade, it is still an open challenge. For instance, there are still requirements with EERs greater than 10\% as the best result among all published works (e.g., \citeReq{\lookingaway} and \citeReq{\hairacrosseyes}). Another point is the fact that the majority of best results for each requirement presented in \autoref{tab:comp} are dominated by private companies. Therefore, there is no detailed explanation about their methods, and it shows the lack of state-of-art methods published as open research. In fact, 12 out of the best results for all the 23 requirements are owned by private companies. Another gap is the low amount of single methods that evaluate all requirements. Only three methods (\biolab, \biotest, \biopass) can evaluate all of them. We believe it is due to the absence of public datasets specialized for the ICAO problem. Finally, we can also conclude that \acl{dl} can be a helpful approach to improve the current results of methods that address the ICAO requirements. One example is the HMAX work, which was able to achieve 0.0\% of EER in two requirements - \citeReq{\framestooheavy} and \citeReq{\framecoveringeyes} - with very low rejection rates.
