\section{Concluding Remarks}

This thesis presented a deep learning-based method developed to evaluate photographic requirements and eye location accuracy of \icao standard, called \methodname. Our method extends undercomplete Convolutional Autoencoders with supervised branches that performs multi-label classification and landmark regression in a collaborative fashion with unsupervised learning. The architecture has three main components: (i) an encoder to encode the input image into a proper 256-D representation shared by (ii) an unsupervised branch to reconstruct the input image; and (iii) supervised branches to assess the requirements as a multi-label problem, determine the eye-center positions as a regression problem, and classify the specific \pixelation requirement as a binary classification task.

We can consider that \methodname presented valuable advances in its research field. First, compared to other encoder-focused architectures available in the literature, the proposed method does not require pre-training and is easy to scale. Secondly, \acl{mtl} was leveraged with different learning techniques (supervised and unsupervised) and tasks (regression and binary/multi-label classification). Regarding Representation Learning, the proposed architecture also employs both generative and discriminative approaches. Therefore, the method learned a functional representation built from related tasks to predict different outputs. Finally, \methodname is the first and only open-source research that evaluates all 23 photographic requirements with considerably low memory consumption and running time.

We evaluated our method using a small amount of unbalanced but stratified data. It comprises a subset of the \ficvtest dataset in conjunction with an \adhoc dataset built especially for ICAO requirements. The network was trained from scratch, and a custom loss function was used in the network optimization process. This function balances the tasks solved by the method, i.e., image reconstruction, landmark localization, and requirements assessment. Additionally, the training was monitored to preserve the model with the highest F-Beta score.

Individually, the \methodname was able to achieve significant results. In training, most of the metrics evaluated in the validation set had a score greater than 90\%. Through the analysis of these metrics, we were able to notice some patterns in the method predictions. For example, it is better to predict the positive class, but the \aclp{fp} are more troublesome than \aclp{fn}. Additionally, in the \acs{ficv} competition, the method presented a substantial performance in most requirements. Nevertheless, unacceptable performance in terms of \acs{eer} ($> 40\%$) were obtained for two requirements and the eye location accuracy was below expectations. Both results will require further work for improvement.

Compared to other methods evaluated by the \acs{ficv} benchmark, the \methodname was able to achieve state-of-art results in 9 out of 23 requirements and a global median EER of 3.3\%. Therefore, the proposed work has the highest amount of best results in a single method compared to all the works presented in the literature or private SDK tools. In terms of EER, the \methodname has the second-best median EER compared to methods that evaluate all requirements. Furthermore, the proposed method is also among the fastest methods to evaluate all requirements on the CPU, taking only 2.7s to evaluate an input image in average. However, our running time still has a spot for improvement since it is highly influenced by the face detector used for preprocessing. The architecture proposed by itself takes only 0.15s to run in the CPU.

We believe that our method was able to outperform the others for the following reasons. First, by using \acs{mtl}, the model can exploit similarities and dependencies between tasks and learn a useful set of features shared among tasks. The features learned from one task can be beneficial for related tasks, particularly when the tasks have overlapping characteristics. This is the case with the \icao standard. Furthermore, \acs{mtl} is known to handle imbalanced and limited data (like the dataset used in this thesis). Thus, we avoided the creation of a model biased to the majority class, which was also enforced by the metrics and regularization techniques applied during training.

Regarding our research question presented in Chapter \ref{sec:introduction}, we can conclude that it has been answered. In terms of efficiency, as cited previously, the proposed method is among the fastest, even without post-optimization. In addition, according to the official FICV benchmark, the method was lightweight, consuming approximately 300 MB of memory. On the other hand, although \methodname was not able to achieve state-of-the-art results for all 23 requirements, we noticed that Deep \acl{mtl} can be an alternative capable of achieving low error rates. Furthermore, by applying the recommendations for future work cited below, the proposed method will likely improve the results.

As future works, we intend to concentrate efforts on three different aspects to improve the results:

\begin{itemize}
\item \textbf{Dataset}: the dataset quality can be improved by increasing (i) the number of images and (ii) the variability of patterns of some requirements (like hat/cap). Thereby, the network can learn more effective descriptors and decrease the EER in these requirements. The most unbalanced requirements may require special attention, and probably more images must be gathered. Furthermore, the dataset labels may be revised to fix possible labeling errors.

\item \textbf{Preprocessing}: as discussed in Chapter \ref{sec:results}, the preprocessing step may have been responsible for some of the errors. First, we can replace the current face detector with a faster and more reliable approach like \cite{faceboxes}. It can help to decrease the detection time (approximately 90\% of total running time) and the Rejection Rates (0.4\% max). Moreover, we must improve the input image provided to the network. For some images, the cropping and resizing steps remove or generate artifacts that can harm network learning. See Figures \ref{fig:variedbgd} and \ref{fig:hairacrosseyes} of Section \ref{sec:ficv_results} for further details. Thus, we need to find a better way to preprocess the input image as a whole without injuring the trade-off between speed and accurate results.

\item \textbf{Method}: some elements of the network can also be considered. It includes, but is not limited to, the architecture and the loss function. For example, the Capsule Neural Networks (CapsNets), proposed by \cite{sabour2017dynamic}, may help the method create hierarchical representation and capture the spatial relationships between different parts of the input image. Also, the Vision Transformers \citep{dosovitskiy2020image} can be used to capture global context information, leading to a better understanding of the image as a whole and avoiding image resizing. Recent techniques like Self-Supervised Learning \citep{doersch2017multi} may also be considered, specially for their ability to reduce annotation costs and handling noisy data. Finally, we intend to test other losses functions specially designed for the multi-label classification task. For instance, the Contrastive Loss \citep{khosla2020supervised}, designed for few-shot learning scenarios, encourages the model to learn to differentiate between similar and dissimilar pairs of data.
\end{itemize}

Finally, the present work won an award and was published in a journal, as follows:

\begin{itemize}
\item \textbf{AI Awards (2\textsuperscript{nd} place)} \citep{aiaward}: The AI Awards is a national award for Artificial Intelligence in Brazil. It is considered the highest award of the Brazilian academy for innovative postgraduate projects involving Artificial Intelligence.

\item \bibentry{icaonet}
\end{itemize}