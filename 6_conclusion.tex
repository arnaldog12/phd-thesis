\section{Concluding Remarks}

This thesis presented a deep learning-based method developed to evaluate photographic requirements and eye location accuracy of \icao standard, called \methodname. Our method extends undercomplete Convolutional Autoencoders with supervised branches that performs multi-label classification and landmark regression in a collaborative fashion with unsupervised learning. The architecture has three main components: (i) an encoder to encode the input image into a proper 256-D representation shared by (ii) an unsupervised branch to reconstruct the input image; and (iii) supervised branches to assess the requirements as a multi-label problem, determine the eye-center positions as a regression problem, and classify the specific \pixelation requirement as a binary classification task.

We can consider that \methodname presented valuable advances in its research field. First, compared to other encoder-focused architectures available in the literature, the proposed method does not require pre-training and is easy to scale. Secondly, \acl{mtl} was leveraged with different learning techniques (supervised and unsupervised) and tasks (regression and binary/multi-label classification). Regarding Representation Learning, the proposed architecture also employs both generative and discriminative approaches. Therefore, the method learned a functional representation built from related tasks to predict different outputs. Finally, \methodname is the first and only open-source research that evaluates all 23 photographic requirements with considerably low memory consumption and running time.

We evaluated our method using a small amount of unbalanced but stratified data. It comprises a subset of the \ficvtest dataset in conjunction with an \adhoc dataset built especially for ICAO requirements. The network was trained from scratch, and a custom loss function was used in the network optimization process. This function balances the tasks solved by the method, i.e., image reconstruction, landmark localization, and requirements assessment. Additionally, the training was monitored to preserve the model with the highest F-Beta score.

Individually, the \methodname was able to achieve significant results. In training, most of the metrics evaluated in the validation set had a score greater than 90\%. Through the analysis of these metrics, we were able to notice some patterns in the method predictions. For example, it is better to predict the positive class, but the \aclp{fp} are more troublesome than \aclp{fn}. Additionally, in the \acs{ficv} competition, the method presented a substantial performance in most requirements. Nevertheless, unacceptable performance in terms of \acs{eer} ($> 40\%$) were obtained for two requirements and the eye location accuracy was below expectations. Both results will require further work for improvement.

Compared to other methods evaluated by the \acs{ficv} benchmark, the \methodname was able to achieve state-of-art results in 9 out of 23 requirements and a global median EER of 3.3\%. Therefore, the proposed work has the highest amount of best results in a single method compared to all the works presented in the literature or private SDK tools. In terms of EER, the \methodname has the second-best median EER compared to methods that evaluate all requirements. Furthermore, the proposed method is also among the fastest methods to evaluate all requirements on the CPU, taking only 2.7s to evaluate an input image in average. However, our running time still has a spot for improvement since it is highly influenced by the face detector used for preprocessing. The architecture proposed by itself takes only 0.15s to run in the CPU.

% trabalho futuros:
% pro FVC: treinar com imagem maior (usando grid ou stride grande), melhorar o dataset (diminuir desbalanceamento). Para os olhos: aumentar o dataset com mais variações
% sem FVC: testar arquiteturas mais atuais (com attention)

As future works, we intend to concentrate efforts on three different aspects to improve the results:

\begin{itemize}
\item \textbf{Dataset}: the dataset quality can be improved by increasing (i) the number of images and (ii) the variability of patterns of some requirements (like hat/cap). Thereby, the network can learn more effective descriptors and decrease the EER in these requirements. The most unbalanced requirements may require special attention, and probably more images must be gathered. Furthermore, the dataset labels may be revised to fix possible labeling errors.

\item \textbf{Preprocessing}: as discussed in Chapter \ref{sec:results}, the preprocessing step may have been responsible for some of the errors. First, we can replace the current face detector with a faster and more reliable approach like \cite{faceboxes}. It can help to decrease the detection time (approximately 90\% of total running time) and the Rejection Rates (0.4\% max). Moreover, we must improve the input image provided to the network. For some images, the cropping and resizing steps remove or generate artifacts that can harm network learning. See Figures \ref{fig:variedbgd} and \ref{fig:hairacrosseyes} of Section \ref{sec:ficv_results} for further details. Thus, we need to find a better way to preprocess the input image as a whole without injuring the trade-off between speed and accurate results.

\item \textbf{Method}: some elements of the network can also be considered. It includes, but is not limited to, the architecture and the loss function. For example, the Capsule Neural Networks (CapsNets), proposed by \cite{sabour2017dynamic}, or Vision Transformers \citep{dosovitskiy2020image}, can be used to leverage the hierarchical relationship between the requirements. Recent techniques like Self-Supervised Learning \citep{doersch2017multi} may also be considered. Finally, we intend to test other losses functions specially designed for the multi-label classification task (e.g., the Contrastive Loss \citep{khosla2020supervised}).
\end{itemize}

Finally, the present work won an award and was published in a journal, as follows:

\begin{itemize}
\item \textbf{AI Awards (2\textsuperscript{nd} place)} \citep{aiaward}: The AI Awards is a national award for Artificial Intelligence in Brazil. It is considered the highest award of the Brazilian academy for innovative postgraduate projects involving Artificial Intelligence.

\item \bibentry{icaonet}
\end{itemize}