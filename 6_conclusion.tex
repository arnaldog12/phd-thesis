\section{Concluding Remarks}

In this chapter, we summarize this proposal in terms of the research and its results. We start by describing the problem and the solution proposal. Then, we present the preliminary evaluation results. Finally, we recap the activities developed until now and propose a schedule to be elaborated up to the end of this Doctoral research.

\subsection{Summary of the Proposed Research}

This proposal presented a deep learning-based method developed to the evaluate photographic requirements of \icao standard, called \methodname. Our method extends undercomplete Convolutional Autoencoders with a supervised branch that performs multi-label classification in a collaborative fashion with unsupervised learning. The architecture has three main components: (i) an encoder to encode the input image into a proper 256-D representation shared by (ii) an unsupervised branch to reconstruct the input image; and (iii) a supervised branch to assess the requirements as a multi-label problem. 

We evaluated our method using a small amount of unbalanced but stratified data. It comprises a subset of the \ficvtest dataset in conjunction with an \adhoc dataset built especially for ICAO requirements. The network was trained from scratch, and a custom loss function was used in the network optimization process. This function balances the two tasks solved by the method, i.e., regression (image reconstruction) and multi-label classification (requirements assessment). Additionally, the training was monitored to preserve the model with the highest F-Beta score.

Individually, the \methodname was able to achieve significant results. In training, most of the metrics evaluated in the validation set had a score greater than 90\%. Through the analysis of these metrics, we were able to notice some patterns in the method predictions. For example, it is better to predict the positive class, but the \aclp{fp} are more troublesome than \aclp{fn}. Additionally, in the FICV competition, the method presented a substantial performance in most requirements. Nevertheless, three requirements had unacceptable performance in terms of \acs{eer} ($> 40\%$) and will require further work to improve their results.

Compared to other methods evaluated by the FICV benchmark, the \methodname was able to achieve state-of-art results in 9 out of 23 requirements with a median EER of 3.3\%. Therefore, the proposed work has the highest amount of best results in a single method compared to all the works presented in the literature or private SDK tools. In terms of EER, the \methodname has the second-best median EER compared to methods that evaluate all requirements. Furthermore, the proposed method is also the fastest one to evaluate all requirements on the CPU, taking only 1.8s to evaluate an input image. However, our running time still has a spot for improvement since it is highly influenced by the face detector used for preprocessing. The architecture proposed by itself takes only 0.15s to run in the CPU.

% trabalho futuros:
% pro FVC: treinar com imagem maior (usando grid ou stride grande), melhorar o dataset (diminuir desbalanceamento). Para os olhos: aumentar o dataset com mais variações
% sem FVC: testar arquiteturas mais atuais (com attention)

As future works, we intend to concentrate efforts on three different aspects to improve the results:

\begin{itemize}
\item \textbf{Dataset}: the dataset quality can be improved by increasing (i) the number of images and (ii) the variability of patterns of some requirements (like hat/cap). Thereby, the network can learn more effective descriptors and decrease the EER in these requirements. The most unbalanced requirements may require special attention, and probably more images must be gathered. Furthermore, the dataset labels may be revised to fix possible labeling errors.

\item \textbf{Preprocessing}: as discussed in Chapter \ref{sec:results}, the preprocessing step may have been responsible for some of the errors. First, we can replace the current face detector with a more fast and reliable approach like \cite{faceboxes}. It can help to decrease the detection time (approximately 90\% of total running time) and the Rejection Rates (0.4\% max). Moreover, we must improve the input image provided to the network. For some images, the cropping and resizing steps remove or generate artifacts that can harm network learning. See Figures \ref{fig:variedbgd} and \ref{fig:hairacrosseyes} of section \ref{sec:ficv_results} for further details. Thus, we need to find a better way to preprocess the input image as a whole without injuring the trade-off between speed and accurate results.

\item \textbf{Method}: some elements of the network can also be considered. It includes, but is not limited to, the architecture and the loss function. For example, the Capsule Neural Networks (CapsNets), proposed by \cite{sabour2017dynamic}, can be used to leverage the hierarchical relationship between the requirements. Recent techniques like Self-Supervised Learning \citep{doersch2017multi} may also be considered. Finally, we intend to test other losses functions specially designed for the multi-label classification task (e.g., the Contrastive Loss \citep{khosla2020supervised}).
\end{itemize}
