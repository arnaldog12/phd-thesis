\section{Preliminary Evaluations} \label{sec:results}

This chapter presents and discusses the preliminary results accomplished by the proposed method so far. First, \methodname results in the training stage are presented, including the loss and metrics computed. Secondly, we discuss the performance of the proposed method in the FICV competition. Then, the \methodname is compared against the best results achieved by the methods shown in Chapter \ref{sec:literature}. Finally, we analyze some visualizations performed over the proposed method to understand its predictions.

\subsection{Training Performance}

In this subsection, we discuss the training performance regarding the \icao requirements followed by the results of eye landmarks detection. We focus on analysing  the loss charts and values of metrics described in Chapter \ref{sec:measures}.

\subsubsection{Requirements}

We start by presenting the results of \methodname during the training phase. The \autoref{fig:losses} shows the loss in training and validation sets for unsupervised and supervised branches individually. As can be seen, the loss of unsupervised branch was lower and smoother than the supervised during the whole training. It was expected since (i) the reconstruction task performed by the unsupervised branch is theoretically more straightforward than the multi-label classification carried out by the supervised branch, and (ii) the unsupervised loss has a higher weight during training (see section \ref{sec:hyperparams} and \autoref{eq:loss-final}). 

A more detailed analysis of our loss curves shows that \methodname presented an appropriate balance between bias and variance. In the case of the unsupervised branch, both train and validation loss are approximately zero and similar to each other. Therefore, the network achieved a notable performance in reconstructing the input image from the shared embeddings. Similarly, even though the losses from the supervised branch are more significant than the unsupervised, they are still close to zero. However, we can notice a more noisy loss curve regarding the validation set. Although such loss curves present a more considerable variance, they keep near the training curve through the epochs. Furthermore, the overfitting was prevented by the regularization techniques employed during training—for example, batch normalization, dropout layers, early stopping, and the architecture itself.

\begin{figure}[ht]
\centering
\subfigure[unsupervised]{\includegraphics[width=0.8\linewidth]{images/graphs/loss_unsupervised.png}}
\hfill
\ContinuedFloat
\subfigure[supervised]{\includegraphics[width=0.8\linewidth]{images/graphs/loss_supervised.png}}
\caption{Loss of training and validation sets for (a) unsupervised and (b) supervised branches. Source: own elaboration.}
\label{fig:losses}
\end{figure}

The final metrics of \methodname can be seen \autoref{tab:metrics}. It can show some meaningful insights about the proposed method. First, it is better to predict the compliant requirements (positive class) than the non-compliant since the Precision and Recall are higher than the \acs{npv} and Specificity. Probably, this is influenced by the unbalanced dataset. Moreover, the \acl{fp} predictions (type-I error) are a more critical problem of \methodname because both Recall and \acs{npv} are greater than Precision and Specificity, respectively. In particular, the Specificity indicates that a reasonable amount of non-compliant requirements are being assigned as compliant. On the other hand, the proposed method achieved considerable values of F-measure and F-beta, which shows a fair balance between Precision and Recall. Finally, the notable \acs{mcc} score (82.78) indicates that the \methodname was able to learn valuable patterns for both compliant/non-compliant requirements even with the unbalancing present in the \adhoc database.

\input{tables/table_metrics}

\subsubsection{Eye Location Accuracy}

In contrast to the intuition, the detection of eye landmarks revealed to be a harder task than the assessment of \icao requirements. Although we tried different approaches, loss functions, and datasets, \methodname was not able to learn detecting the landmarks accurately as other methods. In \autoref{tab:eyes}, we summarize the different aspects we evaluated to train \methodname in the task of landmark detection. All these approaches have in common a new supervised branch added on top of embeddings layer similar to the requirements branch. 

As can be seen in \autoref{tab:eyes}, by using an approach similar to general regression problems (i.e., \acs{mse} as loss function and the metric for Early Stop), we achieved $d_{eye} \in [0;0.1[ = 37.9\%$. Then, we changed the Early Stop metric to be the same as in the \acs{ficv} competition. It improved the results to $41.99\%$. Later, the loss functions was replaced by the Wing Loss, which is a more advanced metric for the problem of landmark detection according to the specialized literature. However, the $d_{eye}$ decreased to $40.21\%$.

We decided to appraise changes in the training and validation datasets as well. First, we removed non-compliant images of \darktintedlenses from the \adhoc dataset. It was done because, for these images, the landmark detector applied to label the dataset (see Section \ref{sec:databaseadhoc}) makes less accurate detections or, in some cases, does not detect the eye landmarks. We can notice in the penultimate row of \autoref{tab:eyes} that it helped to improve the results, but by a small margin.




\input{tables/table_eyes}

\subsection{Results in the FICV Competition} \label{sec:ficv_results}

In \autoref{tab:icaonet-ficv}, we can see the \acl{eer} and Rejection Rate for each FICV dataset. In the \ficvtest, the method proposed was able to achieve a perfect \acs{eer} in eight requirements (08, 11, 12, 16, 23, 24, 28, and 29). In the \ficvofficial dataset, it happened only in the \veiloverface, even though most of the other results are considerably low. 

\input{tables/table_icaonet_ficv}

Three requirements had an \acs{eer} greater than 40\% (10, 14, and 30) in both datasets. In common, they all have a high level of unbalancing (as presented in \autoref{tab:req-dist}). However, other requirements with similar or even worse unbalancing achieved better performance (e.g., 25, 13, or 28). In the case of \pixelation, we credit this bad result to the preprocessing since some high-resolution images are pixelated after the resizing step. Moreover, to easily improve the results of the \otherfacesortoys, we could automatically decrease the score of images with two or more detected faces by our detector. However, the development of post-processing methods is not the primary objective of this work, but it could be considered a future work or be released in the next versions of \methodname.

In terms of Rejection Rates, only four requirements had images rejected during evaluation (08, 15, 17, and 18). According to our implementation, we only reject images for evaluation when a face is not detected. Therefore, such rejections represent the false negatives from the face detector used to preprocess input images (see section \ref{sec:preprocessing} of Chapter \ref{sec:method}). Furthermore, by analyzing these requirements, all of them may hamper face detection in extreme cases. 

We highlight the substantial differences in \acs{eer} between both datasets in \autoref{tab:icaonet-ficv} for requirements 13, 15, and 17. In the case of \washedout, we believe it is caused because all the non-compliant images of this requirement in the \ficvtest belong to the AR database (see \autoref{fig:washedout}). Therefore the pattern learned by the network might not have been generalized to the official database. For the \variedbackground requirement, it may be affected by the cropping applied during preprocessing step of our method (see \autoref{fig:preprocessing}). The cropping can either (i) generate black borders to the input image or (ii) exclude artifacts that introduce variability to the background. According to our analysis to understand our network's output, we could observe that the black borders do not substantially influence the predictions (more details are provided further in section \ref{sec:netviz}). Thus, as can be seen in \autoref{fig:variedbgd}, the artifacts excluded by crop can have a noticeable effect on network learning. Lastly, one possible reason for the \hairacrosseyes requirement can be the resizing operation performed by the preprocessing step (see \autoref{fig:hairacrosseyes}). Since the \adhoc dataset images are mostly of high resolution and they are reduced to 160x160, it can affect the images where thin locks of hair are crossing the eyes region. These cases are not rare to occur in the dataset and, even using the recommended method for image decimation (see section \ref{sec:preprocessing}), the resize may be contributing negatively to the patterns of this requirement. 

\begin{figure}[t]
\centering
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_m-007-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_m-048-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_w-018-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_w-054-1_C40.png}}
\caption{Example of preprocessed non-compliant images from the \washedout requirement. Source: own elaboration.}
\label{fig:washedout}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[original image]{\includegraphics[height=1.5in]{images/varied_background/visio_icao_expotec_56.png}}
\hspace{0.5in}
\subfigure[preprocessed image]{\includegraphics[height=1.5in]{images/varied_background/visio_icao_expotec_56_preprocessed.png}}
\caption{Example of a non-compliant image from the \variedbackground requirement before and after the preprocessing step. Source: own elaboration.}
\label{fig:variedbgd}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[before resize (1715x1715)]{\includegraphics[height=2in]{images/hair_across_eyes/Fall2003_04853d17_preprocessed.jpg}}
\hspace{0.5in}
\subfigure[after resize (160x160)]{\includegraphics[height=2in]{images/hair_across_eyes/Fall2003_04853d17_resized.jpg}}
\caption{Example of a non-compliant image from the \hairacrosseyes requirement before and after the resizing operation performed by the preprocessing step. Source: own elaboration.}
\label{fig:hairacrosseyes}
\end{figure}

The \autoref{fig:eer_unbalancing} shows the \acs{eer} in both FICV datasets (as in \autoref{tab:icaonet-ficv}), but ordering the requirements by the proportion of non-compliant images in the ad-hoc dataset. Through the analysis of this graph, we can make some conclusions. First, as pointed before, the performance on both datasets is similar to each other. It reinforces the premise of FICV that the \ficvtest is a representative set of the \ficvofficial dataset (see section \ref{sec:fvcongoing}). Secondly, there is a moderate correlation between the EER and the degree of unbalancing in our dataset. According to Pearson's correlation, these coefficients are -0.47 and -0.45 for the \ficvtest and \ficvofficial datasets, respectively. In other words, if the proportion of non-compliant images increases, the \acs{eer} tends to decrease. However, it is important to notice that the fifth most unbalanced requirement (\veiloverface), with only 364 non-compliant images (or 6.31\%), is the one that achieved 0.0\% of \acs{eer} in both datasets. Also, these correlations become very weak and positives ($< +0.1$) when computed starting from the seventh most unbalanced requirement (\toodarklight, with 456 non-compliant images or 7.91\%).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{images/graphs/eer_unbalancing.pdf}
\caption{EER by the proportion of non-compliant images for each requirement in ascending order. Source: own elaboration.}
\label{fig:eer_unbalancing}
\end{figure}

\input{tables/table_comp_eyes}

\subsubsection{Analysis of the Worst Requirements}

% Trazer análise dos piores resultados para cá (linha 35: Three requirements had an \acs{eer} greater than 40\%) 
% e mencionar as tentativas de melhoria
% colocar um asterisco no resultado de Pixelation da tabela 6 com uma nota de rodapé, indicando que aquele resultado foi melhorado e citando esse seção

\subsection{Comparison Against Other Methods}

Table \ref{tab:best-results} summarizes the best results by requirement among all the methods shown in Table \ref{tab:comp}. Also, it includes the results of \methodname for easy comparison. All methods were evaluated using the FICV competition benchmark tool and the official dataset (\ficvofficial). As can be seen, the proposed method has the best results in 9 out of 23 requirements of the \icao standard. Thus, the proposed method has the highest amount of best results in terms of requirements. We can also notice that \methodname has low rejection rates, with only four requirements rejecting at most 0.4\% of the evaluated images. The reason behind these rejections is explained in the previous section. 

\input{tables/table_best_results}

There are only four methods with public results published in the \fvcongoing: BioPass Face \citep{fvcVsoft}, BioTest \citep{fvcBioTest}, id3 \citep{fvcICAOCompliance}, and ICAOSDK \citep{fvcSeamfix}. In comparison to them, we have the best results in 11 out of all 23 requirements. Therefore, the \methodname is also the method with the highest amount of best results in terms of requirements in the FICV competition. Additionally, the proposed method has the second-best Median EER (3.3\%).

In terms of performance, the \methodname takes on average 1.8 seconds per image according to the official benchmark results on the FICV competition of the \fvcongoing website. In comparison with methods that evaluate all requirements, the proposed method is the fastest. However, according to our benchmarks, almost 90\% of the \methodname running time is dominated by the face detector (1.6s on average), which is a preprocessing step. On the other hand, the architecture of the proposed network takes only 0.15s of this total time. Furthermore, since the FICV competition runs the benchmarks in CPU-only computers, our network could be even faster by using a GPU. In the future, we intend to change our face detector to a faster alternative so that our total time in CPU will be reduced even more.

\subsection{Network Visualization} \label{sec:netviz}

In addition to the performance results already presented, we decided to apply different techniques to understand the network outputs. First, we analyzed the embeddings learned by the network using algorithms for dimensionality reduction. Secondly, we applied network visualization techniques to understand which image regions are the most relevant according to each requirement. More details can be found in the following paragraphs.

A visualization of the embeddings learned by \methodname is shown in the 3D plots of Figure \ref{fig:embviz}. The embedding dimension was reduced to 3 dimensions and visualized via the PCA \citep{pca} and t-SNE \citep{tsne} methods using TensorBoard\footnote{https://www.tensorflow.org/tensorboard}. Each point in the plot is represented by a face image from the dataset. In the figure, it is possible to see the embeddings allow the formation of cluster-like regions containing face images that share the same semantic resemblance (e.g., varied background, sunglasses, unnatural skin tone, and veil over face). Although the figure shows only three dimensions, it is possible to observe that some dimensions are related to particular ICAO requirements. Such information is relevant to the multi-task classification branch of the \methodname architecture. 

\begin{figure}[ht]
\centering
\subfigure[PCA]{\includegraphics[width=0.75\linewidth]{images/pca.png}}
\subfigure[t-SNE]{\includegraphics[width=0.75\linewidth]{images/tsne.png}}    
\caption{Visualization of the embeddings learned by \methodname. Original embeddings dimensions were reduced to 3D using (a) PCA and (b) t-SNE. Source: own elaboration.}
\label{fig:embviz}
\end{figure}

Figure \ref{fig:shap} contains a visual representation of input images with local region contributions associated with each pose and photograph requirements. The SHAP \citep{shap2018} method was used to create that visualization. The figure provides evidence that the network learned useful representations for most of the requirements. In the fully compliant images (first three rows), the classification output is usually increased by the image regions related to that requirement. For example, in the eye-region dependent requirements (09, 15, 16, 20, 23, 24, and 26), the output is mainly influenced by regions closer to the eyes. Similar behaviors can be observed in requirements related to the mouth (28 and 29), skin (11, 19, and 22), and the image aspect (08, 12, and 13). 

However, we can notice the network could not learn relevant patterns in some requirements like \inkmarked, \pixelation, \framestooheavy, \hatcap, and \otherfacesortoys. For requirements 10, 25, and 30, we believe the low amount of non-compliant images was the most crucial factor in contributing to the worst results of the proposed architecture, found in Table \ref{tab:req-dist}. In the case of \pixelation, a possible cause for the high EER (42.7\%) could be the image resizing step applied in the input images. Finally, the random patterns in \hatcap may show that the variability of head props in our dataset must be insufficient to distinguish them from other patterns. 

\begin{landscape}
\begin{figure*}[t]
\centering
\includegraphics[width=\linewidth]{images/shap.png}
\caption{Visual explanation of \methodname's output using SHAP. The first three images of the first column represent fully compliant images from AR, FRGC, and PUT databases. The remaining rows are composed of images with at least one non-compliant requirement. The higher the SHAP value, the higher the compliance level for that requirement, and vice-versa. SHAP values near zero (white regions) mean no contribution. Source: own elaboration.}
    \label{fig:shap}
\end{figure*}
\end{landscape}
