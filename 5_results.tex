\section{Preliminary Evaluations} \label{sec:results}

This Chapter presents and discuss the preliminary results accomplished by this work during the research. First, \methodname results in training stage are presented, including the loss and metrics computed. Secondly, we discuss the performance of the proposed method in the FICV competition. Then, the \methodname is compared against the best results achieved by the methods shown in Chapter \ref{sec:literature}. Finally, we analyze some visualizations performed over the proposed method to understand its predictions.

\subsection{Training Performance}

We start by presenting the results of \methodname during training phase. The \autoref{fig:losses} shows the loss in training and validation sets for unsupervised and supervised branches individually. As can be seen, the loss of unsupervised branch was lower and smoother than the supervised during the whole training. It was expected since (i) the reconstruction task performed by the unsupervised branch is theoretically easier than the multi-label classification carried out by the supervised branch; and (ii) the unsupervised loss has a higher weight during training (see section \ref{sec:hyperparams} and \autoref{eq:loss-final}). 

A more detailed analysis of our loss curves show that \methodname presented a relevant balance between bias and variance. In the case of the unsupervised branch, both train and validation loss are approximately zero and similar to each other. Therefore, the network achieved a notable performance on reconstructing the input image from the shared embeddings. Similarly, despite of the losses from the supervised branch are bigger than the unsupervised, they are still close to zero. However, we can notice a more noisy loss curve regarding the validation set. Although the points of such loss curve presents a larger variance, in general, they keep nearby the training curve through the epochs. Furthermore, the overfitting was prevented by the regularization techniques employed during training. For example, batch normalization, dropout layers, early stopping, and the architecture itself.

\begin{figure}[ht]
\centering
\subfigure[unsupervised]{\includegraphics[width=0.8\linewidth]{images/graphs/loss_unsupervised.png}}
\hfill
\ContinuedFloat
\subfigure[supervised]{\includegraphics[width=0.8\linewidth]{images/graphs/loss_supervised.png}}
\caption{Loss of training and validation sets for (a) unsupervised and (b) supervised branches.}
\label{fig:losses}
\end{figure}

The final metrics of \methodname can be seen \autoref{tab:metrics}. It can show some important insights about the proposed method. First, it is better predicting the compliant requirements (positive class) than the non-compliant since the Precision and Recall are higher than the \acs{npv} and Specificity. Probably, this is influenced by the unbalanced dataset. Moreover, the \acl{fp} predictions (type-I error) are a more critical problem of \methodname because both Recall and \acs{npv} are greater than Precision and Specificity, respectively. In special, the Specificity indicates that an important amount of non-compliant requirements are being assigned as compliant. On the other hand, the proposed method achieved considerable values of F-measure and F-beta, which shows a fair balance between Precision and Recall. Finally, the notable \acs{mcc} score (82.78) indicates that the \methodname was able to learn useful patterns for both compliant/non-compliant requirements even with the unbalancing present in the ad hoc database.

\input{tables/table_metrics}

\subsection{Results in the FICV competition} \label{sec:ficv_results}

In \autoref{tab:icaonet-ficv}, we can see the \acl{eer} and Rejection Rate for each FICV dataset. In the \ficvtest, the method proposed was able to achieve a perfect \acs{eer} in eight requirements (08, 11, 12, 16, 23, 24, 28, and 29). In the \ficvofficial dataset, it happened only in the \citeReq{\veiloverface}, even though most of the other results are considerable low. 

\input{tables/table_icaonet_ficv}

Three requirements had an \acs{eer} greater than 40\% (10, 14, and 30) in both datasets. In common, they all have a high level of unbalancing (as presented in \autoref{tab:req-dist}). However, other requirements with similar or even worse unbalancing achieved better performance (e.g., 25, 13, or 28). In case of \citeReq{\pixelation}, we credit this bad result to the preprocessing since some high resolution images are pixelated after the resizing step. Moreover, to easily improve the results of the \citeReq{\otherfacesortoys}, we could automatically decrease the score of images with two or more detected faces by our detector. However, the development of post-processing methods are not the primary objective of this work. But, it could be considered as a future work or be released in the next versions of \methodname.

In terms of Rejection Rates, only four requirements had images rejected during evaluation (08, 15, 17, and 18). According to our implementation, we only reject images for evaluation when a face is not detected. Therefore, such rejections represent the false negatives from the face detector used to preprocess input images (see section \ref{sec:preprocessing} of Chapter \ref{sec:method}). Furthermore, by analyzing these requirements, all of them may hamper face detection in extreme cases. 

We highlight the substantial differences in \acs{eer} between both datasets in \autoref{tab:icaonet-ficv} for requirements 13, 15, and 17. In the case of \citeReq{\washedout}, we believe it is caused because all the non-compliant images of this requirement in the \ficvtest belongs to the AR database (see \autoref{fig:washedout}). Therefore the pattern learned by the network might not have been generalized to the official database. For the \citeReq{\variedbackground} requirement, it may be affected by the cropping applied during preprocessing step of our method (see \autoref{fig:preprocessing}). The cropping can either (i) generate black borders to the input image, or (ii) exclude artifacts that introduce variability to the background. According to our analysis to understand our network's output, we could observe that the black borders does not influence substantially the predictions (more details are provided further in section \ref{sec:netviz}). Thus, as can be seen in \autoref{fig:variedbgd}, the artifacts excluded by crop can have a considerable effect on network learning. Lastly, one possible reason for the \citeReq{\hairacrosseyes} requirement can be the resizing operation performed by the preprocessing step. Since the images of the ad hoc dataset are mostly of high resolution and they are reduced to 160x160, it can be affecting the images where thin locks of hair are crossing the eyes region. These cases are not rare to occur in the dataset and, even using the recommended method for image decimation (see section \ref{sec:preprocessing}), the resize may be contributing negatively to the patterns of this requirement. 

\begin{figure}[t]
\centering
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_m-007-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_m-048-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_w-018-1_C40.png}}
\hfill
\subfigure{\includegraphics[width=0.23\linewidth]{images/washed_out/AR_w-054-1_C40.png}}
\caption{Example of preprocessed non-compliant images from the \citeReq{\washedout} requirement.}
\label{fig:washedout}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[original image]{\includegraphics[height=1.5in]{images/varied_background/visio_icao_expotec_56.png}}
\hspace{0.5in}
\subfigure[preprocessed image]{\includegraphics[height=1.5in]{images/varied_background/visio_icao_expotec_56_preprocessed.png}}
\caption{Example of a non-compliant image from the \citeReq{\variedbackground} requirement before and after the preprocessing step.}
\label{fig:variedbgd}
\end{figure}

\begin{figure}[t]
\centering
\subfigure[before resize (1715x1715)]{\includegraphics[height=2in]{images/hair_across_eyes/Fall2003_04853d17_preprocessed.jpg}}
\hspace{0.5in}
\subfigure[after resize (160x160)]{\includegraphics[height=2in]{images/hair_across_eyes/Fall2003_04853d17_resized.jpg}}
\caption{Example of a non-compliant image from the \citeReq{\hairacrosseyes} requirement before and after the resizing operation performed by the preprocessing step.}
\label{fig:hairacrosseyes}
\end{figure}

The \autoref{fig:eer_unbalancing} shows the \acs{eer} in both FICV datasets (as in \autoref{tab:icaonet-ficv}), but ordering the requirements by the proportion of non-compliant images in the ad-hoc dataset. Through the analysis of this graph, we can take some conclusions. First, as pointed before, the performance on both datasets are similar to each other. It reinforces the premise of FICV that the \ficvtest is a representative set of the \ficvofficial dataset (see section \ref{sec:fvcongoing}). Secondly, there is a moderate correlation between the EER and the degree of unbalancing in our dataset. According to the Pearson's correlation, these coefficients are -0.47 and -0.45 for the \ficvtest and \ficvofficial datasets, respectively. In other words, if the proportion of non-compliant images increases, the \acs{eer} tends to decrease. However, it is important to notice that the fifth most unbalanced requirement (\citeReq{\veiloverface}), with only 364 non-compliant images (or 6.31\%), is the one that achieved 0.0\% of \acs{eer} in both datasets. Also, these correlations becomes very weaks and positives ($< +0.1$) when computed starting from the seventh most unbalanced requirement (\citeReq{\toodarklight}, with 456 non-compliant images or 7.91\%).

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\linewidth]{images/graphs/eer_unbalancing.pdf}
\caption{EER by the proportion of non-compliant images for each requirement in ascending order.}
\label{fig:eer_unbalancing}
\end{figure}

\subsection{Comparison against other methods}

Table \ref{tab:best-results} summarizes the best results by requirement among all the methods shown in Table \ref{tab:comp}. Also, it includes the results of \methodname for easy comparison. All methods were evaluated by the benchmark tool of the FICV competition and in the official dataset (\ficvofficial). As can be seen, the proposed method has the best results in 9 out of 23 requirements of \icao standard. Thus, the proposed method has the highest amount of best results in terms of requirements. We can also notice that \methodname has low rejection rates, with only four requirements rejecting at most 0.4\% of the evaluated images. The reason behind these rejections are explained in previous section. 

\input{tables/table_best_results}

There are only four methods with public results published in the \fvcongoing: BioPass Face \citep{fvcVsoft}, BioTest \citep{fvcBioTest}, id3 \citep{fvcICAOCompliance}, and ICAOSDK \citep{fvcSeamfix}. In comparison to them, we have the best results in 11 out of all 23 requirements. Therefore, the \methodname is also the method with the highest amount of best results in terms of requirements in the FICV competition. Additionally, the proposed method has the second-best Median EER (3.3\%).

In terms of performance, the \methodname takes on average 1.8 seconds per image according to the official benchmark results on the FICV competition of the \fvcongoing website. In comparison with methods that evaluate all requirements, the proposed method is the fastest. However, according to our benchmarks, almost 90\% of the \methodname running time is dominated by the face detector (1.6s on average), which is a preprocessing step. On the other hand, the architecture of the proposed network takes only 0.15s of this total time. Furthermore, since the FICV competition runs the benchmarks in CPU-only computers, our network could be even faster by using a GPU. In the future, we intend to change our face detector to a faster alternative so that our total time in CPU will be reduced even more.

\subsection{Network Visualization} \label{sec:netviz}

In addition to the performance results already presented, we decided to apply different techniques to understand the network outputs. First, we analyzed the embeddings learned by the network using algorithms for dimensionality reduction. Secondly, we applied network visualization techniques to understand which image regions are the most relevant according to each requirement. More details can be found in the next paragraphs.

A visualization of the embeddings learned by \methodname is shown in the 3D plots of Figure \ref{fig:embviz}. The embedding dimension was reduced to 3 dimensions and visualized via the PCA \citep{pca} and t-SNE \citep{tsne} methods using TensorBoard\footnote{https://www.tensorflow.org/tensorboard}. Each point in the plot is represented by a face image from the dataset. In the figure, it is possible to see the embeddings allow the formation of cluster-like regions containing face images that share the same semantic resemblance (e.g., varied background, sunglasses, unnatural skin tone, and veil over face). Although the figure shows only three dimensions, it is possible to observe that some dimensions are related to particular ICAO requirements. Such information is relevant to the multi-task classification branch of the \methodname architecture. 

\begin{figure}[ht]
\centering
\subfigure[PCA]{\includegraphics[width=0.8\linewidth]{images/pca.png}}
\subfigure[t-SNE]{\includegraphics[width=0.8\linewidth]{images/tsne.png}}    
\caption{Visualization of the embeddings learned by \methodname. Original embeddings dimensions were reduced to 3D using (a) PCA and (b) t-SNE.}
\label{fig:embviz}
\end{figure}

Figure \ref{fig:shap} contains a visual representation of input images with local region contributions associated with each pose and photograph requirements. The SHAP \citep{shap2018} method was used to create that visualization. The figure provides evidence that the network learned useful representations for most of the requirements. In the fully compliant images (first three rows), the classification output is usually increased by the image regions related to that requirement. For example, in the eye-region dependent requirements (09, 15, 16, 20, 23, 24, and 26), the output is mainly influenced by regions closer to the eyes. Similar behaviors can be observed in requirements related to the mouth (28 and 29), skin (11, 19, and 22), and the image aspect (08, 12, and 13). 

However, we can notice the network was not able to learn relevant patterns in some requirements like \citeReq{\inkmarked}, \citeReq{\pixelation}, \citeReq{\framestooheavy}, \citeReq{\hatcap}, and \citeReq{\otherfacesortoys}. For requirements 10, 25, and 30, we believe the low amount of non-compliant images was the most crucial factor in contributing to the worst results of the proposed architecture, found in Table \ref{tab:req-dist}. In the case of \citeReq{\pixelation}, a possible cause for the high EER (42.7\%) could be the image resizing step applied in the input images. Finally, the random patterns in \citeReq{\hatcap} may show the variability of head props in our dataset must be insufficient to distinguish them from other patterns. 

\begin{landscape}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{images/shap.png}
    \caption{Visual explanation of \methodname's output using SHAP. The first three images of the first column represent fully-compliant images from AR, FRGC, and PUT databases. The remaining rows are composed of images with at least one non-compliant requirement. The higher the SHAP value, the higher the compliance level for that requirement, and vice-versa. SHAP values near zero (white regions) means no contribution.}
    \label{fig:shap}
\end{figure*}
\end{landscape}

% We have already performed tests with one-shot learning approaches (like Siamese Networks) and triplet loss, but the results were worst than the proposed method. 
